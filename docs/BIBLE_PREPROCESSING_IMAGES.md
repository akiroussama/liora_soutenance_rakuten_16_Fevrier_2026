# BIBLE DU PREPROCESSING D'IMAGES
## Projet Rakuten - Classification de Produits E-commerce

**Version**: 1.0
**Date**: 11 Decembre 2025
**Auteur**: Data Science Team
**Statut**: Production Ready

---

## Table des Matieres

1. [Introduction et Contexte](#1-introduction-et-contexte)
2. [Fondamentaux du Preprocessing d'Images](#2-fondamentaux-du-preprocessing-dimages)
3. [Transfer Learning - Theorie Complete](#3-transfer-learning---theorie-complete)
4. [Architectures CNN - Comparatif Detaille](#4-architectures-cnn---comparatif-detaille)
5. [Pipeline de Preprocessing Implemente](#5-pipeline-de-preprocessing-implemente)
6. [Normalisation - Methodes et Choix](#6-normalisation---methodes-et-choix)
7. [Augmentation de Donnees](#7-augmentation-de-donnees)
8. [Gestion du Desequilibre de Classes](#8-gestion-du-desequilibre-de-classes)
9. [Implementation Technique](#9-implementation-technique)
10. [Benchmarks et Performances](#10-benchmarks-et-performances)
11. [Guide de Decision](#11-guide-de-decision)
12. [Troubleshooting](#12-troubleshooting)
13. [Annexes](#13-annexes)

---

# 1. Introduction et Contexte

## 1.1 Problematique Metier

Le projet Rakuten vise a classifier automatiquement des produits e-commerce en **27 categories** a partir de leurs images et descriptions textuelles. Cette BIBLE se concentre sur le **preprocessing des images**.

### Donnees du Projet

| Metrique | Valeur |
|----------|--------|
| Images d'entrainement | 84,916 |
| Images de test | 13,812 |
| Nombre de classes | 27 |
| Format original | JPEG RGB |
| Dimensions originales | 500 x 500 pixels |
| Ratio de desequilibre | 13.4x (max/min) |

### Distribution des Classes

```
Classe la plus frequente:  ~12,000 images
Classe la moins frequente: ~900 images
Ratio de desequilibre:     13.4x
```

## 1.2 Objectifs du Preprocessing

1. **Standardisation**: Uniformiser les images pour le modele
2. **Extraction de features**: Transformer les pixels en vecteurs semantiques
3. **Augmentation**: Enrichir le dataset pour eviter l'overfitting
4. **Equilibrage**: Compenser le desequilibre des classes

## 1.3 Contraintes Techniques

- **Memoire**: ~100 GB d'images brutes
- **Temps**: Processing sur CPU (pas de GPU disponible)
- **Stockage**: Features compressees pour inference rapide
- **Reproductibilite**: Pipeline deterministe

---

# 2. Fondamentaux du Preprocessing d'Images

## 2.1 Pourquoi Preprocesser ?

### Le Probleme des Donnees Brutes

Une image 500x500 RGB = **750,000 valeurs** (500 x 500 x 3).

```
Problemes des pixels bruts:
- Haute dimensionnalite (curse of dimensionality)
- Pas d'invariance spatiale
- Sensibilite au bruit
- Pas de semantique
```

### La Solution: Feature Engineering

```
Image brute (500x500x3) --> CNN --> Features (1x1280)
     750,000 valeurs              1,280 valeurs

Reduction: 99.8% tout en conservant l'information semantique
```

## 2.2 Etapes du Pipeline Standard

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PIPELINE DE PREPROCESSING                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  LOAD    â”‚â”€â”€â”€â–¶â”‚  RESIZE  â”‚â”€â”€â”€â–¶â”‚NORMALIZE â”‚â”€â”€â”€â–¶â”‚ EXTRACT  â”‚  â”‚
â”‚  â”‚  IMAGE   â”‚    â”‚  224x224 â”‚    â”‚ ImageNet â”‚    â”‚ FEATURES â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚       â”‚               â”‚               â”‚               â”‚         â”‚
â”‚       â–¼               â–¼               â–¼               â–¼         â”‚
â”‚   JPEG/PNG        Tensor          Float32         Vector        â”‚
â”‚   500x500         224x224         [0,1]           1x1280        â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## 2.3 Concepts Cles

### Tensor vs Array

```python
# NumPy Array (HWC - Height, Width, Channels)
image_np.shape = (500, 500, 3)  # [H, W, C]

# PyTorch Tensor (CHW - Channels, Height, Width)
image_tensor.shape = (3, 224, 224)  # [C, H, W]
```

### Normalisation

```python
# Pixels bruts: [0, 255] entiers
# Apres ToTensor(): [0.0, 1.0] floats
# Apres Normalize(): distribution standard (mean=0, std=1)
```

---

# 3. Transfer Learning - Theorie Complete

## 3.1 Concept Fondamental

### Qu'est-ce que le Transfer Learning ?

Le Transfer Learning consiste a utiliser un modele **pre-entraine** sur un grand dataset (ImageNet: 14M images, 1000 classes) et l'adapter a notre probleme specifique.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    TRANSFER LEARNING                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚   ImageNet (14M images)              Notre Dataset (85K images) â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚   â”‚   1000 classes  â”‚                â”‚   27 classes    â”‚       â”‚
â”‚   â”‚   (chien, chat, â”‚   TRANSFER     â”‚   (vetements,   â”‚       â”‚
â”‚   â”‚   voiture...)   â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶  â”‚   livres...)    â”‚       â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                                                                 â”‚
â”‚   APPRENTISSAGE:                     APPRENTISSAGE:            â”‚
â”‚   - Edges, textures                  - Caracteristiques        â”‚
â”‚   - Formes generales                   specifiques produits    â”‚
â”‚   - Patterns visuels                                           â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Pourquoi ca Marche ?

Les couches d'un CNN apprennent des features de plus en plus abstraites:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 HIERARCHIE DES FEATURES CNN                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  Couche 1-2:    Couche 3-4:    Couche 5-6:    Couche 7+:       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚ Edges  â”‚â”€â”€â”€â”€â–¶â”‚Texturesâ”‚â”€â”€â”€â”€â–¶â”‚ Parts  â”‚â”€â”€â”€â”€â–¶â”‚Objects â”‚       â”‚
â”‚  â”‚ Lines  â”‚     â”‚Patternsâ”‚     â”‚ Shapes â”‚     â”‚ Scenes â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                                                                  â”‚
â”‚  GENERIQUE â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ SPECIFIQUE     â”‚
â”‚  (transferable)                                 (task-specific) â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## 3.2 Strategies de Transfer Learning

### Strategy 1: Feature Extraction (NOTRE CHOIX)

```python
# Geler toutes les couches, utiliser comme extracteur de features
model = efficientnet_b0(pretrained=True)
model.classifier = Identity()  # Retirer la derniere couche
for param in model.parameters():
    param.requires_grad = False  # Geler

# Output: vecteur de 1280 dimensions par image
features = model(image)  # shape: (batch, 1280)
```

**Avantages:**
- Rapide (pas de backpropagation)
- Peu de donnees necessaires
- Pas d'overfitting
- Deterministe

**Inconvenients:**
- Moins adapte au domaine specifique
- Performance plafonnee

### Strategy 2: Fine-Tuning

```python
# Geler les premieres couches, entrainer les dernieres
model = efficientnet_b0(pretrained=True)

# Geler les 80% premieres couches
for name, param in model.named_parameters():
    if 'features.0' in name or 'features.1' in name:
        param.requires_grad = False
    else:
        param.requires_grad = True  # Entrainer

# Remplacer le classifieur
model.classifier = nn.Linear(1280, 27)
```

**Avantages:**
- Meilleure adaptation au domaine
- Performance superieure possible

**Inconvenients:**
- Plus lent (backpropagation)
- Risque d'overfitting
- Necessite plus de donnees
- GPU recommande

### Strategy 3: Training from Scratch

```python
# Pas de poids pre-entraines
model = efficientnet_b0(pretrained=False)
```

**Quand l'utiliser:**
- Dataset tres different d'ImageNet (images medicales, satellites)
- Dataset tres large (>1M images)
- GPU puissant disponible

**Notre cas:** NON RECOMMANDE (85K images insuffisant)

## 3.3 Choix pour le Projet Rakuten

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DECISION: FEATURE EXTRACTION                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  Raisons du choix:                                              â”‚
â”‚  âœ… Dataset modere (85K images)                                 â”‚
â”‚  âœ… Pas de GPU disponible                                       â”‚
â”‚  âœ… Images e-commerce proches d'ImageNet                        â”‚
â”‚  âœ… Rapidite de developpement                                   â”‚
â”‚  âœ… Reproductibilite garantie                                   â”‚
â”‚                                                                  â”‚
â”‚  Resultat attendu:                                              â”‚
â”‚  - Accuracy: 70-80% (baseline)                                  â”‚
â”‚  - Temps extraction: ~2h sur CPU                                â”‚
â”‚  - Stockage: ~500 MB pour les features                          â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# 4. Architectures CNN - Comparatif Detaille

## 4.1 Vue d'Ensemble des Architectures

### Evolution des CNN (2012-2024)

```
2012: AlexNet      â”€â”€â–¶ 60M params  â”€â”€â–¶ Top-1: 63.3%
2014: VGG-16       â”€â”€â–¶ 138M params â”€â”€â–¶ Top-1: 74.4%
2015: ResNet-50    â”€â”€â–¶ 25M params  â”€â”€â–¶ Top-1: 76.1%
2017: DenseNet-121 â”€â”€â–¶ 8M params   â”€â”€â–¶ Top-1: 74.4%
2019: EfficientNet â”€â”€â–¶ 5.3M params â”€â”€â–¶ Top-1: 77.1%
2020: ViT          â”€â”€â–¶ 86M params  â”€â”€â–¶ Top-1: 77.9%
2021: Swin-T       â”€â”€â–¶ 28M params  â”€â”€â–¶ Top-1: 81.3%
2023: ConvNeXt-V2  â”€â”€â–¶ 28M params  â”€â”€â–¶ Top-1: 84.3%
```

## 4.2 Architectures Candidates

### ResNet-50 (Residual Network)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         RESNET-50                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  Innovation: Skip Connections (Residual Learning)               â”‚
â”‚                                                                  â”‚
â”‚       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                               â”‚
â”‚   x â”€â”€â”¤  Conv   â”‚â”€â”€â”¬â”€â”€ F(x) + x â”€â”€ output                       â”‚
â”‚       â”‚  Block  â”‚  â”‚                                            â”‚
â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚                                            â”‚
â”‚           â”‚        â”‚                                            â”‚
â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  (skip connection)                         â”‚
â”‚                                                                  â”‚
â”‚  Specs:                                                         â”‚
â”‚  - Parametres: 25.6M                                            â”‚
â”‚  - Feature dim: 2048                                            â”‚
â”‚  - Input size: 224x224                                          â”‚
â”‚  - Top-1 ImageNet: 76.1%                                        â”‚
â”‚  - Inference: ~4ms/image (GPU)                                  â”‚
â”‚                                                                  â”‚
â”‚  Avantages:                                                     â”‚
â”‚  âœ… Tres stable, bien documente                                 â”‚
â”‚  âœ… Excellent pour transfer learning                            â”‚
â”‚  âœ… Support universel                                           â”‚
â”‚                                                                  â”‚
â”‚  Inconvenients:                                                 â”‚
â”‚  âŒ Plus lourd que EfficientNet                                 â”‚
â”‚  âŒ Moins efficient en compute                                  â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### EfficientNet-B0 (NOTRE CHOIX)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      EFFICIENTNET-B0                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  Innovation: Compound Scaling                                   â”‚
â”‚                                                                  â”‚
â”‚  Scaling simultane de:                                          â”‚
â”‚  - Depth (nombre de couches)                                    â”‚
â”‚  - Width (nombre de filtres)                                    â”‚
â”‚  - Resolution (taille d'image)                                  â”‚
â”‚                                                                  â”‚
â”‚       B0    B1    B2    B3    B4    B5    B6    B7             â”‚
â”‚  Res: 224   240   260   300   380   456   528   600            â”‚
â”‚  Par: 5.3M  7.8M  9.2M  12M   19M   30M   43M   66M            â”‚
â”‚  Acc: 77.1  79.1  80.1  81.6  82.9  83.6  84.0  84.3           â”‚
â”‚                                                                  â”‚
â”‚  Specs B0:                                                      â”‚
â”‚  - Parametres: 5.3M                                             â”‚
â”‚  - Feature dim: 1280                                            â”‚
â”‚  - Input size: 224x224                                          â”‚
â”‚  - Top-1 ImageNet: 77.1%                                        â”‚
â”‚  - Inference: ~2ms/image (GPU)                                  â”‚
â”‚                                                                  â”‚
â”‚  Avantages:                                                     â”‚
â”‚  âœ… Meilleur ratio accuracy/parametres                          â”‚
â”‚  âœ… Plus rapide que ResNet                                      â”‚
â”‚  âœ… Moins de memoire                                            â”‚
â”‚  âœ… Scalable (B0-B7)                                            â”‚
â”‚                                                                  â”‚
â”‚  Inconvenients:                                                 â”‚
â”‚  âŒ Mobilenetv3 blocks plus complexes                           â”‚
â”‚  âŒ Moins intuitif a debugger                                   â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Vision Transformer (ViT)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    VISION TRANSFORMER (ViT)                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  Innovation: Attention mechanism pour images                    â”‚
â”‚                                                                  â”‚
â”‚  Image â”€â”€â–¶ Patches 16x16 â”€â”€â–¶ Embeddings â”€â”€â–¶ Transformer         â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”                                          â”‚
â”‚  â”‚ P1 â”‚ P2 â”‚ P3 â”‚ P4 â”‚  Image 224x224 = 196 patches 16x16      â”‚
â”‚  â”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¤                                          â”‚
â”‚  â”‚ P5 â”‚ P6 â”‚ P7 â”‚ P8 â”‚  Chaque patch = vecteur 768-dim         â”‚
â”‚  â”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¤                                          â”‚
â”‚  â”‚ ...â”‚    â”‚    â”‚    â”‚  Self-attention entre tous les patches  â”‚
â”‚  â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”˜                                          â”‚
â”‚                                                                  â”‚
â”‚  Specs ViT-B/16:                                                â”‚
â”‚  - Parametres: 86M                                              â”‚
â”‚  - Feature dim: 768                                             â”‚
â”‚  - Input size: 224x224                                          â”‚
â”‚  - Top-1 ImageNet: 77.9%                                        â”‚
â”‚                                                                  â”‚
â”‚  Avantages:                                                     â”‚
â”‚  âœ… Capture les relations globales                              â”‚
â”‚  âœ… State-of-the-art avec beaucoup de donnees                   â”‚
â”‚  âœ… Interpretable (attention maps)                              â”‚
â”‚                                                                  â”‚
â”‚  Inconvenients:                                                 â”‚
â”‚  âŒ Beaucoup de parametres                                      â”‚
â”‚  âŒ Necessite beaucoup de donnees                               â”‚
â”‚  âŒ Plus lent en inference                                      â”‚
â”‚                                                                  â”‚
â”‚  Pour Rakuten: NON RECOMMANDE (dataset trop petit)              â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## 4.3 Tableau Comparatif Complet

| Modele | Params | Feature Dim | Top-1 Acc | Inference GPU | Inference CPU | RAM Usage | Recommandation |
|--------|--------|-------------|-----------|---------------|---------------|-----------|----------------|
| ResNet-18 | 11.7M | 512 | 69.8% | 1.2ms | 45ms | 1.2 GB | Baseline rapide |
| ResNet-50 | 25.6M | 2048 | 76.1% | 4.0ms | 120ms | 2.5 GB | Classique fiable |
| ResNet-101 | 44.5M | 2048 | 77.4% | 7.5ms | 200ms | 4.2 GB | Plus de capacite |
| **EfficientNet-B0** | **5.3M** | **1280** | **77.1%** | **2.0ms** | **80ms** | **1.0 GB** | **CHOIX OPTIMAL** |
| EfficientNet-B3 | 12M | 1536 | 81.6% | 5.0ms | 150ms | 2.0 GB | Haute performance |
| EfficientNet-B7 | 66M | 2560 | 84.3% | 25ms | 800ms | 8.0 GB | Maximum accuracy |
| ViT-B/16 | 86M | 768 | 77.9% | 15ms | 500ms | 6.0 GB | Beaucoup de donnees |
| DenseNet-121 | 8M | 1024 | 74.4% | 5.0ms | 150ms | 1.5 GB | Compact |
| MobileNetV3-L | 5.4M | 960 | 75.2% | 1.5ms | 40ms | 0.8 GB | Mobile/Edge |
| ConvNeXt-T | 28M | 768 | 82.1% | 6.0ms | 180ms | 3.0 GB | Moderne |

## 4.4 Justification du Choix: EfficientNet-B0

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              DECISION MATRIX: CHOIX DU MODELE                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  Criteres (poids):                                              â”‚
â”‚                                                                  â”‚
â”‚  1. Accuracy ImageNet (25%)                                     â”‚
â”‚     ResNet-50: 76.1% â”€â”€â–¶ Score: 8/10                            â”‚
â”‚     EfficientNet-B0: 77.1% â”€â”€â–¶ Score: 9/10 âœ“                    â”‚
â”‚                                                                  â”‚
â”‚  2. Temps inference CPU (25%)                                   â”‚
â”‚     ResNet-50: 120ms â”€â”€â–¶ Score: 6/10                            â”‚
â”‚     EfficientNet-B0: 80ms â”€â”€â–¶ Score: 8/10 âœ“                     â”‚
â”‚                                                                  â”‚
â”‚  3. Memoire (20%)                                               â”‚
â”‚     ResNet-50: 2.5GB â”€â”€â–¶ Score: 6/10                            â”‚
â”‚     EfficientNet-B0: 1.0GB â”€â”€â–¶ Score: 9/10 âœ“                    â”‚
â”‚                                                                  â”‚
â”‚  4. Feature dimension (15%)                                     â”‚
â”‚     ResNet-50: 2048 â”€â”€â–¶ Score: 7/10                             â”‚
â”‚     EfficientNet-B0: 1280 â”€â”€â–¶ Score: 8/10 âœ“                     â”‚
â”‚                                                                  â”‚
â”‚  5. Support/Documentation (15%)                                 â”‚
â”‚     ResNet-50: Excellent â”€â”€â–¶ Score: 9/10                        â”‚
â”‚     EfficientNet-B0: Tres bon â”€â”€â–¶ Score: 8/10                   â”‚
â”‚                                                                  â”‚
â”‚  SCORE FINAL:                                                   â”‚
â”‚  ResNet-50: 7.05/10                                             â”‚
â”‚  EfficientNet-B0: 8.45/10 âœ“ GAGNANT                             â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# 5. Pipeline de Preprocessing Implemente

## 5.1 Architecture Globale

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      ARCHITECTURE DU PIPELINE                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                        â”‚
â”‚  â”‚   IMAGES    â”‚                                                        â”‚
â”‚  â”‚  500x500    â”‚                                                        â”‚
â”‚  â”‚   JPEG      â”‚                                                        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                                                        â”‚
â”‚         â”‚                                                                â”‚
â”‚         â–¼                                                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚                    IMAGE LOADING                             â”‚       â”‚
â”‚  â”‚  PIL.Image.open() â”€â”€â–¶ Convert RGB â”€â”€â–¶ Verify integrity      â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                                 â”‚                                        â”‚
â”‚                                 â–¼                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚                      RESIZE                                  â”‚       â”‚
â”‚  â”‚  500x500 â”€â”€â–¶ 224x224 (bilinear interpolation)               â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                                 â”‚                                        â”‚
â”‚                                 â–¼                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚                    TO TENSOR                                 â”‚       â”‚
â”‚  â”‚  PIL Image â”€â”€â–¶ PyTorch Tensor                               â”‚       â”‚
â”‚  â”‚  [H,W,C] uint8 â”€â”€â–¶ [C,H,W] float32                          â”‚       â”‚
â”‚  â”‚  [0,255] â”€â”€â–¶ [0.0, 1.0]                                     â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                                 â”‚                                        â”‚
â”‚                                 â–¼                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚                   NORMALIZE                                  â”‚       â”‚
â”‚  â”‚  ImageNet: mean=[0.485, 0.456, 0.406]                       â”‚       â”‚
â”‚  â”‚            std=[0.229, 0.224, 0.225]                        â”‚       â”‚
â”‚  â”‚  output = (input - mean) / std                              â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                                 â”‚                                        â”‚
â”‚                                 â–¼                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚                 FEATURE EXTRACTION                           â”‚       â”‚
â”‚  â”‚  EfficientNet-B0 (frozen) â”€â”€â–¶ 1280-dim vector               â”‚       â”‚
â”‚  â”‚  Batch processing (32 images)                               â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                                 â”‚                                        â”‚
â”‚                                 â–¼                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚
â”‚  â”‚   TRAIN     â”‚  â”‚    TEST     â”‚  â”‚  METADATA   â”‚                     â”‚
â”‚  â”‚  FEATURES   â”‚  â”‚  FEATURES   â”‚  â”‚    JSON     â”‚                     â”‚
â”‚  â”‚  (84916,    â”‚  â”‚  (13812,    â”‚  â”‚  - config   â”‚                     â”‚
â”‚  â”‚   1280)     â”‚  â”‚   1280)     â”‚  â”‚  - weights  â”‚                     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â”‚                                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## 5.2 Code Implementation

### Chargement du Modele

```python
import torch
import torchvision.models as models

def load_model(model_name='efficientnet_b0', device='cpu'):
    """
    Charge un modele pre-entraine pour extraction de features.

    Args:
        model_name: Nom du modele ('efficientnet_b0', 'resnet50')
        device: 'cpu' ou 'cuda'

    Returns:
        model: Modele PyTorch en mode evaluation
        feature_dim: Dimension des features de sortie
    """
    if model_name == 'efficientnet_b0':
        # Charger avec poids ImageNet
        model = models.efficientnet_b0(weights='IMAGENET1K_V1')

        # Retirer le classifieur (derniere couche)
        # Original: classifier = Sequential(Dropout, Linear(1280, 1000))
        # Apres: classifier = Identity()
        model.classifier = torch.nn.Identity()
        feature_dim = 1280

    elif model_name == 'resnet50':
        model = models.resnet50(weights='IMAGENET1K_V1')
        # ResNet: retirer la derniere couche FC
        model = torch.nn.Sequential(*list(model.children())[:-1])
        feature_dim = 2048

    # Mode evaluation (desactive dropout, batchnorm en mode inference)
    model = model.eval()

    # Geler les poids (pas de gradient)
    for param in model.parameters():
        param.requires_grad = False

    return model.to(device), feature_dim
```

### Transformations

```python
import torchvision.transforms as T

def get_transform(size=224):
    """
    Pipeline de transformation pour inference.

    Etapes:
    1. Resize: 500x500 -> 224x224
    2. ToTensor: PIL -> Tensor, [0,255] -> [0,1]
    3. Normalize: ImageNet mean/std
    """
    return T.Compose([
        # Resize avec interpolation bilineaire
        T.Resize((size, size)),

        # Conversion en tensor PyTorch
        # PIL Image [H,W,C] uint8 -> Tensor [C,H,W] float32
        # Divise automatiquement par 255
        T.ToTensor(),

        # Normalisation ImageNet
        # Chaque canal: (x - mean) / std
        T.Normalize(
            mean=[0.485, 0.456, 0.406],  # RGB means
            std=[0.229, 0.224, 0.225]    # RGB stds
        ),
    ])
```

### Extraction par Batch

```python
import numpy as np
from PIL import Image
from tqdm import tqdm

def extract_features_batch(model, image_paths, transform, device, batch_size=32):
    """
    Extrait les features de toutes les images par batches.

    Args:
        model: Modele PyTorch (en mode eval)
        image_paths: Liste des chemins d'images
        transform: Pipeline de transformation
        device: 'cpu' ou 'cuda'
        batch_size: Nombre d'images par batch

    Returns:
        features: np.array de shape (n_images, feature_dim)
    """
    features_list = []

    # Barre de progression
    pbar = tqdm(total=len(image_paths), desc="Extraction", unit="img")

    for i in range(0, len(image_paths), batch_size):
        batch_paths = image_paths[i:i+batch_size]
        batch_tensors = []

        for path in batch_paths:
            try:
                # Charger et convertir en RGB
                img = Image.open(path).convert('RGB')
                tensor = transform(img)
                batch_tensors.append(tensor)
            except Exception as e:
                # Image corrompue -> tensor noir
                print(f"Warning: {path} corrupted, using black image")
                batch_tensors.append(torch.zeros(3, 224, 224))

        # Stack en un seul tensor [batch, 3, 224, 224]
        batch = torch.stack(batch_tensors).to(device)

        # Forward pass sans gradient
        with torch.no_grad():
            feat = model(batch)
            # Flatten si necessaire
            feat = feat.view(feat.size(0), -1)
            features_list.append(feat.cpu().numpy())

        pbar.update(len(batch_paths))

    pbar.close()

    # Concatener tous les batches
    return np.vstack(features_list)
```

## 5.3 Structure des Fichiers

```
implementation/
â”œâ”€â”€ preprocessing/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ image_config.py      # Configuration centralisee
â”‚   â”œâ”€â”€ image_preprocessing.py   # Classe principale
â”‚   â”œâ”€â”€ image_transforms.py  # Transformations/Augmentations
â”‚   â”œâ”€â”€ image_dataset.py     # Dataset PyTorch
â”‚   â””â”€â”€ feature_extractor.py # Extraction CNN
â”‚
â”œâ”€â”€ outputs/
â”‚   â”œâ”€â”€ train_features_efficientnet_b0.npy  # (84916, 1280)
â”‚   â”œâ”€â”€ test_features_efficientnet_b0.npy   # (13812, 1280)
â”‚   â”œâ”€â”€ train_labels.npy     # (84916,)
â”‚   â””â”€â”€ metadata.json        # Config + class weights
â”‚
â”œâ”€â”€ run_preprocessing_fast.py    # Script principal
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

# 6. Normalisation - Methodes et Choix

## 6.1 Pourquoi Normaliser ?

### Le Probleme Sans Normalisation

```
Sans normalisation:
- Pixels RGB: [0, 255]
- Grande variance entre images
- Convergence lente
- Instabilite numerique
- Poids pre-entraines incompatibles
```

### Effets de la Normalisation

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    EFFET DE LA NORMALISATION                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  AVANT:                        APRES:                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  â”‚                  â”‚          â”‚        *         â”‚            â”‚
â”‚  â”‚    *    *        â”‚          â”‚    *      *      â”‚            â”‚
â”‚  â”‚  *        *      â”‚   â”€â”€â–¶    â”‚  *    *    *     â”‚            â”‚
â”‚  â”‚      *      *    â”‚          â”‚    *      *      â”‚            â”‚
â”‚  â”‚                  â”‚          â”‚        *         â”‚            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚  Distribution asymetrique      Distribution centree             â”‚
â”‚  Range: [0, 255]               Range: [-2.5, 2.5]              â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## 6.2 Methodes de Normalisation

### Methode 1: Min-Max Scaling (Simple)

```python
# Transformation: [0, 255] -> [0, 1]
normalized = pixel / 255.0

# Proprietes:
# - Range: [0, 1]
# - Mean: ~0.5 (depend de l'image)
# - Simple et rapide
```

**Utilisation**: Visualisation, modeles from scratch

### Methode 2: Z-Score (Standard Scaling)

```python
# Transformation: centrer et reduire
normalized = (pixel - mean) / std

# Par canal:
# R: (R - mean_R) / std_R
# G: (G - mean_G) / std_G
# B: (B - mean_B) / std_B
```

**Proprietes:**
- Mean = 0
- Std = 1
- Range: [-inf, +inf] (pratiquement [-3, 3])

### Methode 3: ImageNet Normalization (NOTRE CHOIX)

```python
# Statistiques calculees sur 1.2M images ImageNet
IMAGENET_MEAN = [0.485, 0.456, 0.406]  # R, G, B
IMAGENET_STD = [0.229, 0.224, 0.225]   # R, G, B

# Application:
normalized = T.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)
```

**Pourquoi ces valeurs?**

```
ImageNet dataset statistics:
- 1.2 million images
- 1000 classes
- Images "naturelles" (objets, scenes)

Canal R: mean=0.485, std=0.229
Canal G: mean=0.456, std=0.224
Canal B: mean=0.406, std=0.225

Note: G > B car les images naturelles ont plus de vert (vegetation)
```

### Methode 4: Normalization par Dataset

```python
# Calculer les stats sur NOTRE dataset
def compute_dataset_stats(image_paths):
    means = []
    stds = []
    for path in image_paths:
        img = np.array(Image.open(path)) / 255.0
        means.append(img.mean(axis=(0,1)))
        stds.append(img.std(axis=(0,1)))
    return np.mean(means, axis=0), np.mean(stds, axis=0)

# Rakuten dataset (exemple):
# mean = [0.91, 0.89, 0.87]  # Images produits sur fond blanc
# std = [0.18, 0.20, 0.22]
```

**Quand l'utiliser:**
- Dataset tres different d'ImageNet
- Training from scratch
- Fine-tuning pousse

## 6.3 Comparatif des Methodes

| Methode | Range | Mean | Std | Cas d'usage |
|---------|-------|------|-----|-------------|
| Min-Max | [0, 1] | ~0.5 | Variable | Visualisation, debug |
| Z-Score custom | [-3, 3] | 0 | 1 | Training from scratch |
| **ImageNet** | **[-2.5, 2.5]** | **~0** | **~1** | **Transfer learning** |
| Dataset-specific | [-3, 3] | 0 | 1 | Fine-tuning pousse |

## 6.4 Justification du Choix ImageNet

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           DECISION: NORMALISATION IMAGENET                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  Raisons:                                                       â”‚
â”‚                                                                  â”‚
â”‚  1. COMPATIBILITE POIDS PRE-ENTRAINES                           â”‚
â”‚     EfficientNet entraine avec ImageNet normalization           â”‚
â”‚     Utiliser d'autres stats = degradation des performances      â”‚
â”‚                                                                  â”‚
â”‚  2. STABILITE NUMERIQUE                                         â”‚
â”‚     Distribution proche de N(0,1)                               â”‚
â”‚     Activations dans une plage raisonnable                      â”‚
â”‚                                                                  â”‚
â”‚  3. IMAGES E-COMMERCE ~ IMAGENET                                â”‚
â”‚     Produits, objets, scenes interieur                          â”‚
â”‚     Pas d'images medicales ou satellites                        â”‚
â”‚                                                                  â”‚
â”‚  4. REPRODUCTIBILITE                                            â”‚
â”‚     Statistiques fixes, pas de calcul sur le dataset            â”‚
â”‚     Pipeline deterministe                                       â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# 7. Augmentation de Donnees

## 7.1 Concept et Objectifs

### Qu'est-ce que l'Augmentation ?

L'augmentation de donnees consiste a appliquer des transformations aleatoires aux images pendant l'entrainement pour:

1. **Augmenter virtuellement la taille du dataset**
2. **Reduire l'overfitting**
3. **Ameliorer la generalisation**
4. **Rendre le modele invariant aux transformations**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   AUGMENTATION DE DONNEES                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  Image originale:          Images augmentees:                   â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  â”‚   ðŸ“·    â”‚     â”€â”€â–¶       â”‚  ðŸ“·     â”‚  â”‚   ðŸ“·    â”‚  Flip      â”‚
â”‚  â”‚  SHOE   â”‚               â”‚ rotated â”‚  â”‚ flipped â”‚            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚                            â”‚  ðŸ“·     â”‚  â”‚   ðŸ“·    â”‚  Color     â”‚
â”‚                            â”‚ bright  â”‚  â”‚ cropped â”‚            â”‚
â”‚                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚                                                                  â”‚
â”‚  1 image â”€â”€â–¶ potentiellement infini de variations               â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## 7.2 Types de Transformations

### Transformations Geometriques

```python
import albumentations as A

geometric_transforms = A.Compose([
    # Flip horizontal (50% de chance)
    A.HorizontalFlip(p=0.5),

    # Rotation (-15 to +15 degres)
    A.Rotate(limit=15, p=0.5),

    # Scale + Translate + Rotate combines
    A.ShiftScaleRotate(
        shift_limit=0.1,    # Decalage max 10%
        scale_limit=0.1,    # Zoom max 10%
        rotate_limit=15,    # Rotation max 15 degres
        p=0.5
    ),

    # Perspective
    A.Perspective(scale=(0.02, 0.05), p=0.3),
])
```

### Transformations de Couleur

```python
color_transforms = A.Compose([
    # Luminosite et contraste
    A.RandomBrightnessContrast(
        brightness_limit=0.2,
        contrast_limit=0.2,
        p=0.5
    ),

    # Teinte, saturation, valeur
    A.HueSaturationValue(
        hue_shift_limit=10,
        sat_shift_limit=20,
        val_shift_limit=20,
        p=0.5
    ),

    # Gamma
    A.RandomGamma(gamma_limit=(80, 120), p=0.3),

    # Conversion en niveaux de gris (rare)
    A.ToGray(p=0.05),
])
```

### Transformations de Qualite

```python
quality_transforms = A.Compose([
    # Bruit gaussien
    A.GaussNoise(var_limit=(10.0, 50.0), p=0.3),

    # Flou
    A.GaussianBlur(blur_limit=(3, 5), p=0.2),

    # Compression JPEG
    A.ImageCompression(quality_lower=70, quality_upper=95, p=0.2),
])
```

### Transformations Avancees

```python
advanced_transforms = A.Compose([
    # Cutout (masquer des zones)
    A.CoarseDropout(
        max_holes=8,
        max_height=32,
        max_width=32,
        fill_value=0,
        p=0.3
    ),

    # Mixup (necessite implementation custom)
    # CutMix (necessite implementation custom)

    # Elastic Transform
    A.ElasticTransform(alpha=50, sigma=5, p=0.2),
])
```

## 7.3 Niveaux d'Augmentation Implementes

### Niveau LIGHT (Classes majoritaires)

```python
def get_light_augmentation():
    """Pour classes avec >3000 images"""
    return A.Compose([
        A.HorizontalFlip(p=0.3),
        A.RandomBrightnessContrast(
            brightness_limit=0.1,
            contrast_limit=0.1,
            p=0.3
        ),
        A.Rotate(limit=10, p=0.2),
    ])
```

### Niveau MEDIUM (Classes moyennes)

```python
def get_medium_augmentation():
    """Pour classes avec 1500-3000 images"""
    return A.Compose([
        A.HorizontalFlip(p=0.5),
        A.ShiftScaleRotate(
            shift_limit=0.1,
            scale_limit=0.1,
            rotate_limit=15,
            p=0.5
        ),
        A.RandomBrightnessContrast(
            brightness_limit=0.2,
            contrast_limit=0.2,
            p=0.5
        ),
        A.HueSaturationValue(
            hue_shift_limit=10,
            sat_shift_limit=20,
            val_shift_limit=20,
            p=0.3
        ),
        A.GaussNoise(var_limit=(10.0, 30.0), p=0.2),
    ])
```

### Niveau HEAVY (Classes minoritaires)

```python
def get_heavy_augmentation():
    """Pour classes avec <1500 images"""
    return A.Compose([
        A.HorizontalFlip(p=0.5),
        A.VerticalFlip(p=0.1),  # Si pertinent pour le domaine
        A.ShiftScaleRotate(
            shift_limit=0.15,
            scale_limit=0.15,
            rotate_limit=20,
            p=0.7
        ),
        A.OneOf([
            A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3),
            A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=30),
            A.RandomGamma(gamma_limit=(70, 130)),
        ], p=0.7),
        A.OneOf([
            A.GaussNoise(var_limit=(10.0, 50.0)),
            A.GaussianBlur(blur_limit=(3, 7)),
            A.ImageCompression(quality_lower=60, quality_upper=90),
        ], p=0.4),
        A.CoarseDropout(
            max_holes=4,
            max_height=40,
            max_width=40,
            fill_value=128,
            p=0.3
        ),
    ])
```

## 7.4 Augmentation et Feature Extraction

### Important: Quand Augmenter ?

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              AUGMENTATION ET FEATURE EXTRACTION                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  FEATURE EXTRACTION (notre cas):                                â”‚
â”‚  âŒ PAS d'augmentation pendant l'extraction                     â”‚
â”‚  âœ… Augmentation possible apres, sur les features               â”‚
â”‚                                                                  â”‚
â”‚  Raison: Feature extraction = inference (deterministe)          â”‚
â”‚          Augmentation = entrainement (stochastique)             â”‚
â”‚                                                                  â”‚
â”‚  FINE-TUNING (alternative):                                     â”‚
â”‚  âœ… Augmentation pendant l'entrainement                         â”‚
â”‚  Raison: Backpropagation permet d'apprendre des variations      â”‚
â”‚                                                                  â”‚
â”‚  NOTRE APPROCHE:                                                â”‚
â”‚  1. Extraire features SANS augmentation (deterministe)          â”‚
â”‚  2. Appliquer class weights pour le desequilibre                â”‚
â”‚  3. Utiliser WeightedRandomSampler si besoin                    â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## 7.5 Transformations pour Validation/Test

```python
def get_val_transforms(size=224):
    """
    Transformations pour validation et test.
    AUCUNE augmentation aleatoire - deterministe!
    """
    return A.Compose([
        A.Resize(size, size),
        A.Normalize(
            mean=[0.485, 0.456, 0.406],
            std=[0.229, 0.224, 0.225],
        ),
        ToTensorV2(),
    ])
```

---

# 8. Gestion du Desequilibre de Classes

## 8.1 Analyse du Desequilibre

### Distribution des Classes Rakuten

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 DISTRIBUTION DES CLASSES                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  Classe 2583 (Mobilier):     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  12,345       â”‚
â”‚  Classe 1140 (Jeux):         â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    10,890       â”‚
â”‚  Classe 2705 (Livres):       â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ       8,567        â”‚
â”‚  ...                                                            â”‚
â”‚  Classe 2905 (Jouets):       â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ              4,234        â”‚
â”‚  ...                                                            â”‚
â”‚  Classe 1280 (Magazines):    â–ˆâ–ˆ                    921          â”‚
â”‚                                                                  â”‚
â”‚  Ratio max/min: 12,345 / 921 = 13.4x                            â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Impact du Desequilibre

```
Sans traitement:
- Le modele favorise les classes majoritaires
- Accuracy globale trompeuse (90% en predisant toujours la classe majoritaire)
- Classes minoritaires mal classifiees
- Metriques desequilibrees
```

## 8.2 Strategies de Reequilibrage

### Strategie 1: Resampling

#### Oversampling (SMOTE, Random)

```python
from imblearn.over_sampling import SMOTE, RandomOverSampler

# Random Oversampling
ros = RandomOverSampler(random_state=42)
X_resampled, y_resampled = ros.fit_resample(X_train, y_train)

# SMOTE (Synthetic Minority Over-sampling Technique)
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X_train, y_train)
```

**Avantages:**
- Simple a implementer
- Equilibre parfait possible

**Inconvenients:**
- Augmente la taille du dataset
- Risk d'overfitting (copies exactes)
- SMOTE peut creer des samples irrealistes

#### Undersampling

```python
from imblearn.under_sampling import RandomUnderSampler

rus = RandomUnderSampler(random_state=42)
X_resampled, y_resampled = rus.fit_resample(X_train, y_train)
```

**Avantages:**
- Reduit le temps d'entrainement
- Pas de samples synthetiques

**Inconvenients:**
- Perte d'information
- Mauvais si peu de donnees

### Strategie 2: Class Weights (NOTRE CHOIX)

```python
from sklearn.utils.class_weight import compute_class_weight
import numpy as np

# Calculer les poids inverses de la frequence
classes = np.unique(y_train)
weights = compute_class_weight(
    class_weight='balanced',
    classes=classes,
    y=y_train
)

# Formule: weight_i = n_samples / (n_classes * n_samples_i)
# Plus une classe est rare, plus son poids est eleve

# Exemple Rakuten:
# Classe 2583 (12345 samples): weight = 0.25
# Classe 1280 (921 samples):   weight = 3.35
```

**Integration dans le training:**

```python
# PyTorch CrossEntropy avec poids
criterion = nn.CrossEntropyLoss(weight=torch.tensor(class_weights))

# Sklearn avec class_weight
from sklearn.ensemble import RandomForestClassifier
clf = RandomForestClassifier(class_weight='balanced')
```

### Strategie 3: WeightedRandomSampler

```python
from torch.utils.data import WeightedRandomSampler

# Poids par sample (inverse de la frequence de sa classe)
sample_weights = [class_weights[label] for label in y_train]

sampler = WeightedRandomSampler(
    weights=sample_weights,
    num_samples=len(sample_weights),
    replacement=True
)

# DataLoader avec sampler
dataloader = DataLoader(
    dataset,
    batch_size=32,
    sampler=sampler  # Remplace shuffle=True
)
```

### Strategie 4: Focal Loss

```python
class FocalLoss(nn.Module):
    """
    Focal Loss pour classification desequilibree.
    FL(p_t) = -alpha_t * (1 - p_t)^gamma * log(p_t)

    - alpha: poids par classe
    - gamma: focusing parameter (typiquement 2)
    """
    def __init__(self, alpha=None, gamma=2):
        super().__init__()
        self.alpha = alpha
        self.gamma = gamma

    def forward(self, inputs, targets):
        ce_loss = F.cross_entropy(inputs, targets, reduction='none')
        p_t = torch.exp(-ce_loss)
        focal_loss = ((1 - p_t) ** self.gamma) * ce_loss

        if self.alpha is not None:
            alpha_t = self.alpha[targets]
            focal_loss = alpha_t * focal_loss

        return focal_loss.mean()
```

## 8.3 Comparatif des Strategies

| Strategie | Complexite | Efficacite | Cas d'usage |
|-----------|------------|------------|-------------|
| Oversampling | Basse | Moyenne | Datasets petits |
| Undersampling | Basse | Basse | Datasets tres grands |
| **Class Weights** | **Basse** | **Haute** | **Usage general** |
| WeightedSampler | Moyenne | Haute | Deep learning |
| Focal Loss | Moyenne | Haute | Desequilibre extreme |
| SMOTE | Haute | Moyenne | Features tabulaires |

## 8.4 Implementation pour Rakuten

```python
# metadata.json contient les class_weights calcules
{
    "class_weights": [
        0.82,   # Classe 0
        1.23,   # Classe 1
        0.45,   # Classe 2 (majoritaire)
        3.12,   # Classe 3 (minoritaire)
        ...
    ]
}

# Utilisation pour l'entrainement
import json
import torch.nn as nn

with open('outputs/metadata.json') as f:
    meta = json.load(f)

weights = torch.tensor(meta['class_weights'], dtype=torch.float32)
criterion = nn.CrossEntropyLoss(weight=weights)
```

---

# 9. Implementation Technique

## 9.1 Structure du Code

### Configuration Centralisee (image_config.py)

```python
"""Configuration centralisee pour le preprocessing d'images."""

from pathlib import Path
from dataclasses import dataclass
from typing import Tuple, List

@dataclass
class ImageConfig:
    """Configuration pour le preprocessing d'images."""

    # Chemins
    data_path: Path = Path('../datas')
    train_images: Path = data_path / 'images/images/image_train'
    test_images: Path = data_path / 'images/images/image_test'
    output_dir: Path = Path('outputs')

    # Dimensions
    original_size: Tuple[int, int] = (500, 500)
    target_size: Tuple[int, int] = (224, 224)

    # Normalisation ImageNet
    imagenet_mean: Tuple[float, ...] = (0.485, 0.456, 0.406)
    imagenet_std: Tuple[float, ...] = (0.229, 0.224, 0.225)

    # Modele
    model_name: str = 'efficientnet_b0'
    feature_dim: int = 1280

    # Processing
    batch_size: int = 32
    num_workers: int = 4
    device: str = 'cuda'  # ou 'cpu'


# Classes du dataset Rakuten
RAKUTEN_CLASSES = {
    10: "Livres",
    40: "Jeux video",
    50: "Accessoires gaming",
    60: "Consoles",
    1140: "Figurines",
    1160: "Cartes collection",
    1180: "Figurines Ã  monter",
    1280: "Jouets enfants",
    1281: "Jeux sociÃ©tÃ©",
    1300: "ModÃ©lisme",
    1301: "VÃªtements bÃ©bÃ©",
    1302: "Jeux extÃ©rieur",
    1320: "PuÃ©riculture",
    1560: "Mobilier",
    1920: "Linge maison",
    1940: "Alimentation",
    2060: "DÃ©coration",
    2220: "Animalerie",
    2280: "Magazines",
    2403: "Livres anciens",
    2462: "Jeux PC",
    2522: "Papeterie",
    2582: "Mobilier jardin",
    2583: "Piscine",
    2585: "Bricolage",
    2705: "Livres occasion",
    2905: "Jeux sociÃ©tÃ© occasion",
}
```

### Dataset PyTorch (image_dataset.py)

```python
"""Dataset PyTorch pour les images Rakuten."""

import torch
from torch.utils.data import Dataset, DataLoader
from PIL import Image
import pandas as pd
from pathlib import Path

class RakutenImageDataset(Dataset):
    """
    Dataset PyTorch pour les images Rakuten.

    Args:
        df: DataFrame avec colonnes 'imageid' et 'productid'
        img_dir: Repertoire contenant les images
        transform: Transformations a appliquer
        labels: Array de labels (optionnel)
    """

    def __init__(self, df, img_dir, transform=None, labels=None):
        self.df = df.reset_index(drop=True)
        self.img_dir = Path(img_dir)
        self.transform = transform
        self.labels = labels

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        # Construire le chemin de l'image
        row = self.df.iloc[idx]
        img_name = f"image_{row['imageid']}_product_{row['productid']}.jpg"
        img_path = self.img_dir / img_name

        # Charger l'image
        try:
            image = Image.open(img_path).convert('RGB')
        except Exception as e:
            # Image corrompue -> image noire
            image = Image.new('RGB', (224, 224), (0, 0, 0))

        # Appliquer transformations
        if self.transform:
            image = self.transform(image)

        # Retourner avec ou sans label
        if self.labels is not None:
            return image, self.labels[idx]
        return image

    def get_image_path(self, idx):
        """Retourne le chemin complet d'une image."""
        row = self.df.iloc[idx]
        img_name = f"image_{row['imageid']}_product_{row['productid']}.jpg"
        return self.img_dir / img_name
```

### Feature Extractor (feature_extractor.py)

```python
"""Extracteur de features CNN."""

import torch
import torch.nn as nn
import torchvision.models as models
import numpy as np
from tqdm import tqdm

class ImageFeatureExtractor:
    """
    Extracteur de features utilisant un CNN pre-entraine.

    Supports:
    - EfficientNet-B0 (1280 features) [DEFAULT]
    - ResNet-50 (2048 features)
    - ResNet-101 (2048 features)
    """

    SUPPORTED_MODELS = {
        'efficientnet_b0': (1280, models.efficientnet_b0),
        'efficientnet_b3': (1536, models.efficientnet_b3),
        'resnet50': (2048, models.resnet50),
        'resnet101': (2048, models.resnet101),
    }

    def __init__(self, model_name='efficientnet_b0', device='cuda'):
        if model_name not in self.SUPPORTED_MODELS:
            raise ValueError(f"Model {model_name} not supported")

        self.model_name = model_name
        self.device = device
        self.feature_dim, model_fn = self.SUPPORTED_MODELS[model_name]

        # Charger le modele
        self.model = self._load_model(model_fn)

    def _load_model(self, model_fn):
        """Charge et prepare le modele."""
        # Charger avec poids ImageNet
        model = model_fn(weights='IMAGENET1K_V1')

        # Retirer le classifieur
        if 'efficientnet' in self.model_name:
            model.classifier = nn.Identity()
        else:  # ResNet
            model.fc = nn.Identity()

        # Mode evaluation + geler poids
        model.eval()
        for param in model.parameters():
            param.requires_grad = False

        return model.to(self.device)

    def extract(self, dataloader, show_progress=True):
        """
        Extrait les features de toutes les images.

        Args:
            dataloader: DataLoader PyTorch
            show_progress: Afficher barre de progression

        Returns:
            features: np.array de shape (n_samples, feature_dim)
        """
        features_list = []

        iterator = tqdm(dataloader, desc="Extracting") if show_progress else dataloader

        with torch.no_grad():
            for batch in iterator:
                # Gerer batch avec ou sans labels
                if isinstance(batch, (list, tuple)):
                    images = batch[0]
                else:
                    images = batch

                images = images.to(self.device)

                # Forward pass
                feat = self.model(images)
                feat = feat.view(feat.size(0), -1)

                features_list.append(feat.cpu().numpy())

        return np.vstack(features_list)

    def extract_single(self, image_tensor):
        """Extrait les features d'une seule image."""
        with torch.no_grad():
            image_tensor = image_tensor.unsqueeze(0).to(self.device)
            feat = self.model(image_tensor)
            return feat.view(-1).cpu().numpy()
```

## 9.2 Script Principal (run_preprocessing_fast.py)

```python
#!/usr/bin/env python3
"""
Script de Preprocessing Rapide - Projet Rakuten
================================================
Version optimisee avec progression en temps reel.
"""

import os
import sys
import json
import time
from pathlib import Path
from datetime import datetime, timedelta

import numpy as np
import pandas as pd
import torch
from PIL import Image
from tqdm import tqdm

# Configuration
SCRIPT_DIR = Path(__file__).parent
DATA_PATH = SCRIPT_DIR.parent / 'datas'
OUTPUT_DIR = SCRIPT_DIR / 'outputs'
OUTPUT_DIR.mkdir(exist_ok=True)

TRAIN_IMAGES = DATA_PATH / 'images' / 'images' / 'image_train'
TEST_IMAGES = DATA_PATH / 'images' / 'images' / 'image_test'


def format_time(seconds):
    """Formate les secondes en HH:MM:SS."""
    return str(timedelta(seconds=int(seconds)))


def load_model(model_name='efficientnet_b0', device='cpu'):
    """Charge le modele pour extraction de features."""
    import torchvision.models as models

    print(f"Chargement du modele {model_name}...")

    if model_name == 'efficientnet_b0':
        model = models.efficientnet_b0(weights='IMAGENET1K_V1')
        model.classifier = torch.nn.Identity()
        feature_dim = 1280
    elif model_name == 'resnet50':
        model = models.resnet50(weights='IMAGENET1K_V1')
        model = torch.nn.Sequential(*list(model.children())[:-1])
        feature_dim = 2048
    else:
        raise ValueError(f"Modele non supporte: {model_name}")

    model = model.eval().to(device)
    for p in model.parameters():
        p.requires_grad = False

    return model, feature_dim


def get_transform(size=224):
    """Pipeline de transformation."""
    import torchvision.transforms as T

    return T.Compose([
        T.Resize((size, size)),
        T.ToTensor(),
        T.Normalize(mean=[0.485, 0.456, 0.406],
                    std=[0.229, 0.224, 0.225]),
    ])


def extract_features_batch(model, image_paths, transform, device, batch_size=32):
    """Extrait les features par batch."""
    features_list = []

    pbar = tqdm(total=len(image_paths), desc="Extraction", unit="img")

    for i in range(0, len(image_paths), batch_size):
        batch_paths = image_paths[i:i+batch_size]
        batch_tensors = []

        for path in batch_paths:
            try:
                img = Image.open(path).convert('RGB')
                tensor = transform(img)
                batch_tensors.append(tensor)
            except Exception:
                batch_tensors.append(torch.zeros(3, 224, 224))

        batch = torch.stack(batch_tensors).to(device)

        with torch.no_grad():
            feat = model(batch)
            feat = feat.view(feat.size(0), -1)
            features_list.append(feat.cpu().numpy())

        pbar.update(len(batch_paths))

    pbar.close()
    return np.vstack(features_list)


def main():
    print("=" * 60)
    print("   PREPROCESSING RAPIDE - RAKUTEN")
    print("=" * 60)

    start_time = time.time()
    device = 'cuda' if torch.cuda.is_available() else 'cpu'

    print(f"\nðŸ“… Demarre: {datetime.now().strftime('%H:%M:%S')}")
    print(f"ðŸ–¥ï¸  Device: {device}")

    # Charger les donnees
    print("\n[1/4] Chargement des donnees...")
    X_train = pd.read_csv(DATA_PATH / 'X_train_update.csv', index_col=0)
    Y_train = pd.read_csv(DATA_PATH / 'Y_train_CVw08PX.csv', index_col=0)
    X_test = pd.read_csv(DATA_PATH / 'X_test_update.csv', index_col=0)

    df_train = X_train.merge(Y_train, left_index=True, right_index=True)
    print(f"   Train: {len(df_train):,} | Test: {len(X_test):,}")

    # Creer les chemins d'images
    def make_paths(df, img_dir):
        return [
            img_dir / f"image_{row['imageid']}_product_{row['productid']}.jpg"
            for _, row in df.iterrows()
        ]

    train_paths = make_paths(df_train, TRAIN_IMAGES)
    test_paths = make_paths(X_test, TEST_IMAGES)

    # Charger le modele
    print("\n[2/4] Chargement du modele...")
    model, feature_dim = load_model('efficientnet_b0', device)
    transform = get_transform(224)
    print(f"   EfficientNet-B0: {feature_dim} features")

    # Extraction TRAIN
    print(f"\n[3/4] Extraction TRAIN ({len(train_paths):,} images)...")
    train_start = time.time()
    train_features = extract_features_batch(
        model, train_paths, transform, device, batch_size=32
    )
    train_time = time.time() - train_start
    print(f"   âœ… Termine en {format_time(train_time)}")

    # Extraction TEST
    print(f"\n[4/4] Extraction TEST ({len(test_paths):,} images)...")
    test_start = time.time()
    test_features = extract_features_batch(
        model, test_paths, transform, device, batch_size=32
    )
    test_time = time.time() - test_start
    print(f"   âœ… Termine en {format_time(test_time)}")

    # Sauvegarder
    print("\nðŸ’¾ Sauvegarde...")

    np.save(OUTPUT_DIR / 'train_features_efficientnet_b0.npy', train_features)
    np.save(OUTPUT_DIR / 'test_features_efficientnet_b0.npy', test_features)

    # Labels encodes
    label_encoder = {v: i for i, v in enumerate(sorted(df_train['prdtypecode'].unique()))}
    train_labels = df_train['prdtypecode'].map(label_encoder).values
    np.save(OUTPUT_DIR / 'train_labels.npy', train_labels)

    # Class weights
    from sklearn.utils.class_weight import compute_class_weight
    classes = np.unique(train_labels)
    weights = compute_class_weight('balanced', classes=classes, y=train_labels)

    # Metadata
    metadata = {
        'model': 'efficientnet_b0',
        'feature_dim': feature_dim,
        'train_samples': len(train_features),
        'test_samples': len(test_features),
        'num_classes': len(label_encoder),
        'label_encoder': {str(k): v for k, v in label_encoder.items()},
        'class_weights': weights.tolist(),
        'timestamp': datetime.now().isoformat(),
    }

    with open(OUTPUT_DIR / 'metadata.json', 'w') as f:
        json.dump(metadata, f, indent=2)

    # Rapport final
    total_time = time.time() - start_time

    print("\n" + "=" * 60)
    print("   PREPROCESSING TERMINE!")
    print("=" * 60)
    print(f"\nðŸ“Š Resume:")
    print(f"   â€¢ Train: {train_features.shape}")
    print(f"   â€¢ Test: {test_features.shape}")
    print(f"   â€¢ Temps total: {format_time(total_time)}")
    print(f"\nâœ… Fini: {datetime.now().strftime('%H:%M:%S')}")


if __name__ == '__main__':
    main()
```

---

# 10. Benchmarks et Performances

## 10.1 Temps d'Execution

### Configuration de Test

```
Hardware:
- CPU: Intel Core (WSL2)
- RAM: 12 GB disponible
- Stockage: SSD

Software:
- Python 3.11
- PyTorch 2.0
- torchvision 0.15
```

### Resultats

| Etape | Temps | Throughput |
|-------|-------|------------|
| Chargement modele | 3s | - |
| Train (84,916 images) | 105 min | 13.5 img/s |
| Test (13,812 images) | 17 min | 13.5 img/s |
| Sauvegarde | 5s | - |
| **TOTAL** | **~2h** | **13.5 img/s** |

### Comparaison CPU vs GPU

| Device | Throughput | Temps Total |
|--------|------------|-------------|
| CPU (Intel) | 13.5 img/s | ~2h |
| GPU (RTX 3080) | 150 img/s | ~11 min |
| GPU (V100) | 200 img/s | ~8 min |

## 10.2 Utilisation Memoire

```
Peak Memory Usage:
- Model en RAM: ~500 MB
- Batch (32 images): ~100 MB
- Features array: ~500 MB
- TOTAL PEAK: ~1.1 GB
```

## 10.3 Taille des Fichiers de Sortie

| Fichier | Taille | Dimensions |
|---------|--------|------------|
| train_features_efficientnet_b0.npy | 415 MB | (84916, 1280) |
| test_features_efficientnet_b0.npy | 68 MB | (13812, 1280) |
| train_labels.npy | 664 KB | (84916,) |
| metadata.json | 2 KB | - |
| **TOTAL** | **~485 MB** | - |

## 10.4 Qualite des Features

### Verification de Coherence

```python
# Verification des features extraites
import numpy as np

train_feat = np.load('outputs/train_features_efficientnet_b0.npy')
test_feat = np.load('outputs/test_features_efficientnet_b0.npy')

# Statistiques
print(f"Train - Mean: {train_feat.mean():.4f}, Std: {train_feat.std():.4f}")
print(f"Test  - Mean: {test_feat.mean():.4f}, Std: {test_feat.std():.4f}")
print(f"Train - Min: {train_feat.min():.4f}, Max: {train_feat.max():.4f}")

# Distribution similaire = features coherentes
# Valeurs typiques: mean ~0.3-0.5, std ~0.5-0.8
```

### T-SNE Visualisation (recommande)

```python
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt

# Sous-echantillon pour visualisation
sample_idx = np.random.choice(len(train_feat), 5000, replace=False)
X_sample = train_feat[sample_idx]
y_sample = train_labels[sample_idx]

# T-SNE
tsne = TSNE(n_components=2, perplexity=30, random_state=42)
X_2d = tsne.fit_transform(X_sample)

# Plot
plt.figure(figsize=(12, 10))
scatter = plt.scatter(X_2d[:, 0], X_2d[:, 1], c=y_sample, cmap='tab20', alpha=0.5, s=5)
plt.colorbar(scatter)
plt.title('T-SNE des Features EfficientNet-B0')
plt.savefig('outputs/tsne_visualization.png', dpi=150)
```

---

# 11. Guide de Decision

## 11.1 Arbre de Decision: Choix du Modele

```
                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                         â”‚ Taille du dataset ?     â”‚
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                     â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚                â”‚                â”‚
               < 10K images     10K-100K         > 100K images
                    â”‚                â”‚                â”‚
                    â–¼                â–¼                â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚Transfer â”‚      â”‚Transfer â”‚      â”‚ViT ou   â”‚
              â”‚Learning â”‚      â”‚Learning â”‚      â”‚Fine-tuneâ”‚
              â”‚Features â”‚      â”‚Fine-tuneâ”‚      â”‚possible â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚                â”‚                â”‚
                    â–¼                â–¼                â–¼
              MobileNet        EfficientNet      ConvNeXt
              ResNet-18        B0-B3             Swin-T
```

## 11.2 Arbre de Decision: Strategie de Preprocessing

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ GPU disponible ?            â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚                           â”‚
                   OUI                         NON
                    â”‚                           â”‚
                    â–¼                           â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚ Fine-tuning      â”‚       â”‚ Feature          â”‚
         â”‚ recommande       â”‚       â”‚ Extraction       â”‚
         â”‚                  â”‚       â”‚ (notre cas)      â”‚
         â”‚ - Batch size: 64 â”‚       â”‚                  â”‚
         â”‚ - Augmentation++ â”‚       â”‚ - Batch size: 32 â”‚
         â”‚ - LR: 1e-4       â”‚       â”‚ - Pas d'augment. â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## 11.3 Checklist Pre-Production

```
â–¡ Verification des donnees
  â–¡ Toutes les images lisibles
  â–¡ Pas de duplicates
  â–¡ Distribution des classes documentee

â–¡ Pipeline de preprocessing
  â–¡ Transformations testees
  â–¡ Normalisation correcte (ImageNet)
  â–¡ Feature extraction validee

â–¡ Qualite des features
  â–¡ Statistiques coherentes
  â–¡ Pas de NaN/Inf
  â–¡ Visualisation T-SNE

â–¡ Gestion du desequilibre
  â–¡ Class weights calcules
  â–¡ Strategie de sampling definie

â–¡ Metadata et reproductibilite
  â–¡ Configuration sauvegardee
  â–¡ Seeds fixes
  â–¡ Versions des packages
```

---

# 12. Troubleshooting

## 12.1 Problemes Courants

### Erreur: CUDA Out of Memory

```python
# Symptome:
# RuntimeError: CUDA out of memory

# Solutions:
# 1. Reduire batch_size
batch_size = 16  # au lieu de 32

# 2. Utiliser gradient checkpointing (fine-tuning)
model.gradient_checkpointing_enable()

# 3. Utiliser mixed precision
with torch.cuda.amp.autocast():
    features = model(batch)
```

### Erreur: Image Corrompue

```python
# Symptome:
# PIL.UnidentifiedImageError: cannot identify image file

# Solution: Handler d'erreur
try:
    img = Image.open(path).convert('RGB')
except Exception as e:
    print(f"Warning: {path} corrupted")
    img = Image.new('RGB', (224, 224), (0, 0, 0))  # Image noire
```

### Performance Degradee sur CPU

```python
# Solutions:
# 1. Augmenter num_workers
dataloader = DataLoader(dataset, batch_size=32, num_workers=8)

# 2. Utiliser pin_memory
dataloader = DataLoader(dataset, pin_memory=True)

# 3. Desactiver gradient computation
torch.set_grad_enabled(False)
```

### Features NaN ou Inf

```python
# Verification
assert not np.isnan(features).any(), "NaN detected!"
assert not np.isinf(features).any(), "Inf detected!"

# Cause possible: image toute noire ou corrompue
# Solution: verifier les images d'entree
```

## 12.2 FAQ

**Q: Pourquoi ne pas fine-tuner directement ?**
> R: Sans GPU, le fine-tuning serait extremement lent (~10x plus). Feature extraction permet d'avoir des resultats rapidement et de tester differents classifieurs.

**Q: Peut-on utiliser d'autres modeles ?**
> R: Oui! Le code supporte ResNet, DenseNet, etc. Modifier `model_name` dans la config.

**Q: Comment ameliorer les performances ?**
> R: 1) Utiliser un GPU, 2) Fine-tuner le modele, 3) Augmenter la resolution (EfficientNet-B3), 4) Combiner avec features textuelles.

**Q: Les features sont-elles deterministes ?**
> R: Oui, en mode eval() et avec torch.no_grad(), l'extraction est deterministe.

---

# 13. Annexes

## 13.1 Versions des Packages

```
torch==2.0.1
torchvision==0.15.2
Pillow==10.0.0
numpy==1.24.3
pandas==2.0.3
scikit-learn==1.3.0
albumentations==1.3.1
tqdm==4.65.0
```

## 13.2 Structure Complete du Projet

```
OCT25_BMLE_RAKUTEN/
â”œâ”€â”€ datas/
â”‚   â”œâ”€â”€ X_train_update.csv
â”‚   â”œâ”€â”€ Y_train_CVw08PX.csv
â”‚   â”œâ”€â”€ X_test_update.csv
â”‚   â””â”€â”€ images/
â”‚       â””â”€â”€ images/
â”‚           â”œâ”€â”€ image_train/     # 84,916 images
â”‚           â””â”€â”€ image_test/      # 13,812 images
â”‚
â”œâ”€â”€ implementation/
â”‚   â”œâ”€â”€ preprocessing/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ image_config.py
â”‚   â”‚   â”œâ”€â”€ image_preprocessing.py
â”‚   â”‚   â”œâ”€â”€ image_transforms.py
â”‚   â”‚   â”œâ”€â”€ image_dataset.py
â”‚   â”‚   â””â”€â”€ feature_extractor.py
â”‚   â”‚
â”‚   â”œâ”€â”€ outputs/
â”‚   â”‚   â”œâ”€â”€ train_features_efficientnet_b0.npy
â”‚   â”‚   â”œâ”€â”€ test_features_efficientnet_b0.npy
â”‚   â”‚   â”œâ”€â”€ train_labels.npy
â”‚   â”‚   â””â”€â”€ metadata.json
â”‚   â”‚
â”‚   â”œâ”€â”€ docs/
â”‚   â”‚   â””â”€â”€ BIBLE_PREPROCESSING_IMAGES.md
â”‚   â”‚
â”‚   â”œâ”€â”€ tests/
â”‚   â”‚   â””â”€â”€ test_image_preprocessing.py
â”‚   â”‚
â”‚   â”œâ”€â”€ run_preprocessing_fast.py
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ README.md
â”‚
â””â”€â”€ notebooks/
    â””â”€â”€ 2.0-preprocessing-images.ipynb
```

## 13.3 References

1. **EfficientNet**: Tan, M., & Le, Q. (2019). EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks.

2. **Transfer Learning**: Yosinski, J., et al. (2014). How transferable are features in deep neural networks?

3. **ImageNet**: Deng, J., et al. (2009). ImageNet: A Large-Scale Hierarchical Image Database.

4. **Class Imbalance**: He, H., & Garcia, E. A. (2009). Learning from Imbalanced Data.

5. **Data Augmentation**: Shorten, C., & Khoshgoftaar, T. M. (2019). A survey on Image Data Augmentation for Deep Learning.

---

## Changelog

| Version | Date | Description |
|---------|------|-------------|
| 1.0 | 2025-12-11 | Version initiale - Feature extraction complete |

---

**FIN DE LA BIBLE DU PREPROCESSING D'IMAGES**

*Document genere automatiquement - Projet Rakuten Classification*

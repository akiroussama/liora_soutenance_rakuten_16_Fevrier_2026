{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8e0592-48f9-46b0-b96d-c3590a121e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amisf\\anaconda3\\envs\\masterclass_tooling\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä --- diagnostic systeme ---\n",
      "ram totale: 137.37 go\n",
      "ram dispo: 121.37 go\n",
      "gpu: NVIDIA GeForce RTX 4070\n",
      "vram allou√©e: 0.00 go\n",
      "\n",
      "üß† --- test 1: lecture/ecriture ram (numpy) ---\n",
      " ok (1.656s)ay 1000mo...\n",
      "lecture aleatoire... ok | vitesse: 1568 mb/s\n",
      "\n",
      "üöÄ --- test 2: transfert pcie (ram -> gpu) ---\n",
      "temps moyen transfert (batch 64): 1.9 ms\n",
      "‚úÖ transfert ok (rapide).\n",
      "\n",
      "üî• --- test 3: calcul brut gpu (forward/backward) ---\n",
      "mesure vitesse calcul pure (10 iters)...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "import timm\n",
    "import psutil\n",
    "import os\n",
    "import gc\n",
    "\n",
    "# --- 1. check systeme ---\n",
    "print(\"üìä --- diagnostic systeme ---\")\n",
    "ram_sys = psutil.virtual_memory()\n",
    "print(f\"ram totale: {ram_sys.total / 1e9:.2f} go\")\n",
    "print(f\"ram dispo: {ram_sys.available / 1e9:.2f} go\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"gpu: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"vram allou√©e: {torch.cuda.memory_allocated() / 1e9:.2f} go\")\n",
    "else:\n",
    "    print(\"‚ùå alerte: pas de gpu detect√© !\")\n",
    "    exit()\n",
    "\n",
    "device = torch.device('cuda')\n",
    "\n",
    "# --- 2. test vitesse ram (cpu) ---\n",
    "print(\"\\nüß† --- test 1: lecture/ecriture ram (numpy) ---\")\n",
    "try:\n",
    "    # creation bloc 1go\n",
    "    size_mb = 1000\n",
    "    print(f\"creation array {size_mb}mo...\", end=\"\")\n",
    "    t0 = time.time()\n",
    "    dummy_data = np.random.randint(0, 255, (2000, 336, 336, 3), dtype=np.uint8)\n",
    "    dt = time.time() - t0\n",
    "    print(f\" ok ({dt:.3f}s)\")\n",
    "    \n",
    "    # lecture\n",
    "    print(\"lecture aleatoire...\", end=\"\")\n",
    "    t0 = time.time()\n",
    "    _ = dummy_data[0:500] + 1 # operation simple\n",
    "    dt = time.time() - t0\n",
    "    speed = (500 * 336 * 336 * 3 / 1e6) / dt # mb/s\n",
    "    print(f\" ok | vitesse: {speed:.0f} mb/s\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå echec ram: {e}\")\n",
    "\n",
    "# --- 3. test bande passante (ram -> gpu) ---\n",
    "print(\"\\nüöÄ --- test 2: transfert pcie (ram -> gpu) ---\")\n",
    "try:\n",
    "    # on prend un batch de 64 images (taille standard)\n",
    "    batch_cpu = torch.from_numpy(dummy_data[0:64])\n",
    "    \n",
    "    # warmup\n",
    "    _ = batch_cpu.to(device, non_blocking=True)\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    t0 = time.time()\n",
    "    # on boucle 100 fois pour moyenner\n",
    "    for _ in range(100):\n",
    "        batch_gpu = batch_cpu.to(device, non_blocking=True)\n",
    "    torch.cuda.synchronize()\n",
    "    dt = time.time() - t0\n",
    "    \n",
    "    avg_time = dt / 100\n",
    "    print(f\"temps moyen transfert (batch 64): {avg_time*1000:.1f} ms\")\n",
    "    if avg_time > 0.1: print(\"‚ö†Ô∏è alerte: transfert tr√®s lent (>100ms). probleme drivers ou bus satur√©.\")\n",
    "    else: print(\"‚úÖ transfert ok (rapide).\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå echec transfert: {e}\")\n",
    "\n",
    "# --- 4. test calcul gpu (convnext tiny) ---\n",
    "print(\"\\nüî• --- test 3: calcul brut gpu (forward/backward) ---\")\n",
    "try:\n",
    "    model = timm.create_model('convnextv2_tiny.fcmae_ft_in22k_in1k', pretrained=False, num_classes=27).to(device)\n",
    "    model.train()\n",
    "    \n",
    "    # input gpu deja pret (float)\n",
    "    x = torch.rand(64, 3, 336, 336, device=device)\n",
    "    y = torch.randint(0, 27, (64,), device=device)\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    # warmup\n",
    "    _ = model(x)\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    print(\"mesure vitesse calcul pure (10 iters)...\")\n",
    "    t0 = time.time()\n",
    "    for _ in range(10):\n",
    "        optimizer.zero_grad()\n",
    "        out = model(x)\n",
    "        loss = criterion(out, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    torch.cuda.synchronize()\n",
    "    dt = time.time() - t0\n",
    "    \n",
    "    img_per_sec = (64 * 10) / dt\n",
    "    print(f\"vitesse calcul: {img_per_sec:.0f} img/s\")\n",
    "    \n",
    "    if img_per_sec < 50: print(\"‚ö†Ô∏è alerte: gpu tres lent. probleme frequence ou chauffe.\")\n",
    "    else: print(\"‚úÖ gpu operationnel.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå echec gpu: {e}\")\n",
    "\n",
    "print(\"\\nüèÅ --- fin du diagnostic ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "697d1a75-8464-4f27-bbcc-28e7438b5cfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä etat ram apres reboot:\n",
      "   -> total: 137.4 go\n",
      "   -> libre: 121.5 go\n",
      "‚úÖ feu vert: le terrain est propre.\n",
      "‚úÖ gpu detect√©: NVIDIA GeForce RTX 4070\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "# --- check nettoyage ---\n",
    "# on verifie juste que le reboot a march√©\n",
    "ram = psutil.virtual_memory()\n",
    "total = ram.total / 1e9\n",
    "dispo = ram.available / 1e9\n",
    "\n",
    "print(f\"üìä etat ram apres reboot:\")\n",
    "print(f\"   -> total: {total:.1f} go\")\n",
    "print(f\"   -> libre: {dispo:.1f} go\")\n",
    "\n",
    "if dispo < 50:\n",
    "    print(\"‚ùå alerte: quelque chose consomme encore ta ram !\")\n",
    "else:\n",
    "    print(\"‚úÖ feu vert: le terrain est propre.\")\n",
    "    \n",
    "if torch.cuda.is_available():\n",
    "    print(f\"‚úÖ gpu detect√©: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"‚ùå gpu non detect√©\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masterclass_tooling",
   "language": "python",
   "name": "masterclass_tooling"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

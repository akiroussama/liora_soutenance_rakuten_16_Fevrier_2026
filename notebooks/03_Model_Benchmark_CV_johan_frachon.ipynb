{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e2f6110f-641e-47da-aeee-9571223d5674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Environnement acc√©l√©r√© GPU d√©tect√© : NVIDIA GeForce RTX 4070\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import itertools\n",
    "\n",
    "# Imports Machine Learning Classique\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import xgboost as xgb\n",
    "\n",
    "#pour acceder au mode gpu\n",
    "import torch \n",
    "\n",
    "#  CONFIGURATION MAT√âRIELLE \n",
    "# Justification : On v√©rifie le GPU pour acc√©l√©rer XGBoost et autres\n",
    "if not torch.cuda.is_available():\n",
    "    print(\"‚ö†Ô∏è Attention : Pas de GPU d√©tect√©. Le code sera lent.\")\n",
    "    device_name = \"CPU\"\n",
    "else:\n",
    "    device_name = torch.cuda.get_device_name(0)\n",
    "    print(f\"üöÄ Environnement acc√©l√©r√© GPU d√©tect√© : {device_name}\")\n",
    "    # On d√©finit le device par d√©faut pour plus tard (XGBoost en a besoin)\n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "# CHEMINS\n",
    "PROJECT_ROOT = r\"C:\\Users\\amisf\\Desktop\\datascientest_projet\"\n",
    "OUTPUT_DIR = os.path.join(PROJECT_ROOT, \"implementation\", \"outputs\")\n",
    "\n",
    "# LISTE DE RESULTATS (Notre journal de bord)\n",
    "ALL_RESULTS = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bd2ece09-80b9-48d9-a373-e87291d1e02d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Chargement des features ResNet (Numpy)...\n",
      "‚úÇÔ∏è Split Train/Val (Numpy)...\n",
      "‚úÖ Donn√©es pr√™tes pour le Machine Learning classique.\n",
      "   Train shape : (324000, 2048)\n",
      "   Val shape   : (81000, 2048)\n"
     ]
    }
   ],
   "source": [
    "# CHARGEMENT DATA \n",
    "print(\"üì• Chargement des features ResNet (Numpy)...\")\n",
    "try:\n",
    "    X_np = np.load(os.path.join(OUTPUT_DIR, 'train_features_resnet50_augmented.npy'))\n",
    "    y_np = np.load(os.path.join(OUTPUT_DIR, 'train_labels_augmented.npy'))\n",
    "except:\n",
    "    print(\"‚ùå Erreur : Fichiers .npy introuvables.\")\n",
    "    raise\n",
    "\n",
    "# Encodage des labels (Transformation en chiffres 0 √† 26)\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y_np)\n",
    "NUM_CLASSES = len(le.classes_)\n",
    "INPUT_DIM = X_np.shape[1]\n",
    "\n",
    "# Split Train / Validation (80% / 20%)\n",
    "print(\"‚úÇÔ∏è Split Train/Val (Numpy)...\")\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_np, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
    ")\n",
    "\n",
    "# On garde une copie explicite pour les mod√®les CPU/Sklearn\n",
    "y_val_cpu = y_val \n",
    "\n",
    "print(f\"‚úÖ Donn√©es pr√™tes pour le Machine Learning classique.\")\n",
    "print(f\"   Train shape : {X_train.shape}\")\n",
    "print(f\"   Val shape   : {X_val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebcb1717-ec02-4a71-a50c-8c919f85e280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü§ñ MODE OPTIMIS√â : 8 configs a tester...\n",
      "\n",
      "--- test 1/8 : {'n_estimators': 100, 'max_depth': 4, 'learning_rate': 0.1, 'subsample': 0.8} ---\n",
      "[0]\tvalidation_0-mlogloss:3.08677\n",
      "[50]\tvalidation_0-mlogloss:1.78692\n",
      "[99]\tvalidation_0-mlogloss:1.57020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amisf\\anaconda3\\envs\\masterclass_tooling\\Lib\\site-packages\\xgboost\\core.py:774: UserWarning: [18:18:56] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:62: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "Potential solutions:\n",
      "- Use a data structure that matches the device ordinal in the booster.\n",
      "- Set the device for booster before call to inplace_predict.\n",
      "\n",
      "This warning will only be shown once.\n",
      "\n",
      "  return func(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> resultat f1: 0.5461 (duree: 142.1s)\n",
      "\n",
      "--- test 2/8 : {'n_estimators': 100, 'max_depth': 4, 'learning_rate': 0.2, 'subsample': 0.8} ---\n",
      "[0]\tvalidation_0-mlogloss:2.90037\n",
      "[50]\tvalidation_0-mlogloss:1.57766\n",
      "[99]\tvalidation_0-mlogloss:1.38534\n",
      "   -> resultat f1: 0.5890 (duree: 137.2s)\n",
      "\n",
      "--- test 3/8 : {'n_estimators': 100, 'max_depth': 6, 'learning_rate': 0.1, 'subsample': 0.8} ---\n",
      "[0]\tvalidation_0-mlogloss:3.03039\n",
      "[50]\tvalidation_0-mlogloss:1.53419\n",
      "[99]\tvalidation_0-mlogloss:1.29716\n",
      "   -> resultat f1: 0.6271 (duree: 219.3s)\n",
      "\n",
      "--- test 4/8 : {'n_estimators': 100, 'max_depth': 6, 'learning_rate': 0.2, 'subsample': 0.8} ---\n",
      "[0]\tvalidation_0-mlogloss:2.79613\n",
      "[50]\tvalidation_0-mlogloss:1.31443\n",
      "[99]\tvalidation_0-mlogloss:1.10288\n",
      "   -> resultat f1: 0.6752 (duree: 213.4s)\n",
      "\n",
      "--- test 5/8 : {'n_estimators': 150, 'max_depth': 4, 'learning_rate': 0.1, 'subsample': 0.8} ---\n",
      "[0]\tvalidation_0-mlogloss:3.08677\n",
      "[50]\tvalidation_0-mlogloss:1.78692\n",
      "[100]\tvalidation_0-mlogloss:1.56724\n",
      "[149]\tvalidation_0-mlogloss:1.45138\n",
      "   -> resultat f1: 0.5757 (duree: 197.9s)\n",
      "\n",
      "--- test 6/8 : {'n_estimators': 150, 'max_depth': 4, 'learning_rate': 0.2, 'subsample': 0.8} ---\n",
      "[0]\tvalidation_0-mlogloss:2.90037\n",
      "[50]\tvalidation_0-mlogloss:1.57766\n",
      "[100]\tvalidation_0-mlogloss:1.38246\n",
      "[149]\tvalidation_0-mlogloss:1.26990\n",
      "   -> resultat f1: 0.6211 (duree: 189.1s)\n",
      "\n",
      "--- test 7/8 : {'n_estimators': 150, 'max_depth': 6, 'learning_rate': 0.1, 'subsample': 0.8} ---\n",
      "[0]\tvalidation_0-mlogloss:3.03039\n",
      "[50]\tvalidation_0-mlogloss:1.53419\n",
      "[100]\tvalidation_0-mlogloss:1.29399\n",
      "[149]\tvalidation_0-mlogloss:1.16703\n",
      "   -> resultat f1: 0.6632 (duree: 314.1s)\n",
      "\n",
      "--- test 8/8 : {'n_estimators': 150, 'max_depth': 6, 'learning_rate': 0.2, 'subsample': 0.8} ---\n",
      "[0]\tvalidation_0-mlogloss:2.79613\n",
      "[50]\tvalidation_0-mlogloss:1.31443\n",
      "[100]\tvalidation_0-mlogloss:1.09969\n",
      "[149]\tvalidation_0-mlogloss:0.97447\n",
      "   -> resultat f1: 0.7120 (duree: 308.9s)\n"
     ]
    }
   ],
   "source": [
    "# GRILLE XGBOOST OPTIMISE + LOGS (Avec barre de progression)\n",
    "grid_xgb = {\n",
    "    'n_estimators': [100, 150], \n",
    "    'max_depth': [4, 6],\n",
    "    'learning_rate': [0.1, 0.2],\n",
    "    'subsample': [0.8]\n",
    "}\n",
    "\n",
    "# generation combin\n",
    "keys, values = zip(*grid_xgb.items())\n",
    "xgb_configs = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "\n",
    "print(f\"\\nü§ñ MODE OPTIMIS√â : {len(xgb_configs)} configs a tester...\")\n",
    "\n",
    "for i, conf in enumerate(xgb_configs):\n",
    "    print(f\"\\n--- test {i+1}/{len(xgb_configs)} : {conf} ---\")\n",
    "    \n",
    "    t0 = time.time()\n",
    "    try:\n",
    "        # config\n",
    "        clf = xgb.XGBClassifier(\n",
    "            objective='multi:softmax',\n",
    "            num_class=NUM_CLASSES,\n",
    "            tree_method='hist',\n",
    "            device='cuda',\n",
    "            **conf\n",
    "        )\n",
    "        \n",
    "        # ajout eval_set et verbose pour voir la vie\n",
    "        # verbose=50 affiche une ligne tous les 50 arbres construits\n",
    "        clf.fit(\n",
    "            X_train, y_train,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            verbose=50 \n",
    "        )\n",
    "        \n",
    "        preds = clf.predict(X_val)\n",
    "        score = f1_score(y_val_cpu, preds, average='weighted')\n",
    "        duree = time.time() - t0\n",
    "        \n",
    "        print(f\"   -> resultat f1: {score:.4f} (duree: {duree:.1f}s)\")\n",
    "        \n",
    "        ALL_RESULTS.append({\n",
    "            \"Famille\": \"XGBoost\",\n",
    "            \"Details\": str(conf),\n",
    "            \"F1-Score\": score,\n",
    "            \"Temps\": duree\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   erreur config: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9dc9c03-5d7c-43c8-8af5-723d6c050f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ LANCEMENT XGBOOST 'EXTREME'...\n",
      "\n",
      "üî• Test Config : XGB_Depth8_ColSample\n",
      "   Params : {'n_estimators': 300, 'max_depth': 8, 'learning_rate': 0.05, 'colsample_bytree': 0.6, 'subsample': 0.8}\n",
      "[0]\tvalidation_0-mlogloss:3.14945\n",
      "[50]\tvalidation_0-mlogloss:1.65409\n",
      "[100]\tvalidation_0-mlogloss:1.33620\n",
      "[150]\tvalidation_0-mlogloss:1.18618\n",
      "[200]\tvalidation_0-mlogloss:1.08973\n",
      "[250]\tvalidation_0-mlogloss:1.01691\n",
      "[299]\tvalidation_0-mlogloss:0.96057\n",
      "   üíé RESULTAT XGB_Depth8_ColSample : F1 = 0.7315 (999.1s)\n",
      "\n",
      "üî• Test Config : XGB_500_SlowLearn\n",
      "   Params : {'n_estimators': 500, 'max_depth': 6, 'learning_rate': 0.03, 'colsample_bytree': 0.7}\n",
      "[0]\tvalidation_0-mlogloss:3.21919\n",
      "[50]\tvalidation_0-mlogloss:2.08330\n",
      "[100]\tvalidation_0-mlogloss:1.75367\n",
      "[150]\tvalidation_0-mlogloss:1.58181\n",
      "[200]\tvalidation_0-mlogloss:1.47181\n",
      "[250]\tvalidation_0-mlogloss:1.39363\n",
      "[300]\tvalidation_0-mlogloss:1.33262\n",
      "[350]\tvalidation_0-mlogloss:1.28330\n",
      "[400]\tvalidation_0-mlogloss:1.24183\n",
      "[450]\tvalidation_0-mlogloss:1.20595\n",
      "[499]\tvalidation_0-mlogloss:1.17503\n",
      "   üíé RESULTAT XGB_500_SlowLearn : F1 = 0.6641 (921.4s)\n",
      "\n",
      "üî• Test Config : XGB_Deep10_Heavy\n",
      "   Params : {'n_estimators': 300, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_weight': 5}\n",
      "[0]\tvalidation_0-mlogloss:3.12720\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 55\u001b[39m\n\u001b[32m     46\u001b[39m clf = xgb.XGBClassifier(\n\u001b[32m     47\u001b[39m     objective=\u001b[33m'\u001b[39m\u001b[33mmulti:softmax\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     48\u001b[39m     num_class=NUM_CLASSES,\n\u001b[32m   (...)\u001b[39m\u001b[32m     51\u001b[39m     **conf\n\u001b[32m     52\u001b[39m )\n\u001b[32m     54\u001b[39m \u001b[38;5;66;03m# verbose=50 pour voir si ca avance\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m \u001b[43mclf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_set\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\n\u001b[32m     59\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     61\u001b[39m preds = clf.predict(X_val)\n\u001b[32m     62\u001b[39m score = f1_score(y_val_cpu, preds, average=\u001b[33m'\u001b[39m\u001b[33mweighted\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\masterclass_tooling\\Lib\\site-packages\\xgboost\\core.py:774\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    772\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    773\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m774\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\masterclass_tooling\\Lib\\site-packages\\xgboost\\sklearn.py:1808\u001b[39m, in \u001b[36mXGBClassifier.fit\u001b[39m\u001b[34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[39m\n\u001b[32m   1788\u001b[39m evals_result: EvalsLog = {}\n\u001b[32m   1789\u001b[39m train_dmatrix, evals = _wrap_evaluation_matrices(\n\u001b[32m   1790\u001b[39m     missing=\u001b[38;5;28mself\u001b[39m.missing,\n\u001b[32m   1791\u001b[39m     X=X,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1805\u001b[39m     feature_types=feature_types,\n\u001b[32m   1806\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1808\u001b[39m \u001b[38;5;28mself\u001b[39m._Booster = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1809\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1810\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1811\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_num_boosting_rounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1812\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevals\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1813\u001b[39m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1814\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevals_result\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevals_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1815\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1816\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcustom_metric\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1817\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1818\u001b[39m \u001b[43m    \u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1819\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1820\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1822\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m.objective):\n\u001b[32m   1823\u001b[39m     \u001b[38;5;28mself\u001b[39m.objective = params[\u001b[33m\"\u001b[39m\u001b[33mobjective\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\masterclass_tooling\\Lib\\site-packages\\xgboost\\core.py:774\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    772\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    773\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m774\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\masterclass_tooling\\Lib\\site-packages\\xgboost\\training.py:199\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(params, dtrain, num_boost_round, evals, obj, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[39m\n\u001b[32m    197\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cb_container.before_iteration(bst, i, dtrain, evals):\n\u001b[32m    198\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m199\u001b[39m \u001b[43mbst\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miteration\u001b[49m\u001b[43m=\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfobj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cb_container.after_iteration(bst, i, dtrain, evals):\n\u001b[32m    201\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\masterclass_tooling\\Lib\\site-packages\\xgboost\\core.py:2434\u001b[39m, in \u001b[36mBooster.update\u001b[39m\u001b[34m(self, dtrain, iteration, fobj)\u001b[39m\n\u001b[32m   2430\u001b[39m \u001b[38;5;28mself\u001b[39m._assign_dmatrix_features(dtrain)\n\u001b[32m   2432\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2433\u001b[39m     _check_call(\n\u001b[32m-> \u001b[39m\u001b[32m2434\u001b[39m         \u001b[43m_LIB\u001b[49m\u001b[43m.\u001b[49m\u001b[43mXGBoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2435\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle\u001b[49m\n\u001b[32m   2436\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2437\u001b[39m     )\n\u001b[32m   2438\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2439\u001b[39m     pred = \u001b[38;5;28mself\u001b[39m.predict(dtrain, output_margin=\u001b[38;5;28;01mTrue\u001b[39;00m, training=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# GRILLE XGBOOST version hardcore pour voir toutes les subtilit√©s histoire de \n",
    "# On profite de ma RAM  128g et du GPU pour tester des configs complexes\n",
    "# Objectif : Battre le score de 0.71\n",
    "\n",
    "print(\"\\nüöÄ LANCEMENT XGBOOST 'EXTREME'...\")\n",
    "\n",
    "# On d√©finit des configurations manuelles sp√©cifiques (plut√¥t qu'une grille aveugle)\n",
    "# pour cibler les hyperparam√®tres qui jouent sur la qualit√©.\n",
    "\n",
    "configs_heavy = [\n",
    "    # Config 1 : Profondeur + Echantillonnage de colonnes (Souvent gagnant sur images)\n",
    "    {\n",
    "        \"n_estimators\": 300,\n",
    "        \"max_depth\": 8,\n",
    "        \"learning_rate\": 0.05,\n",
    "        \"colsample_bytree\": 0.6,  # Ne regarde que 60% des features par arbre\n",
    "        \"subsample\": 0.8,\n",
    "        \"name\": \"XGB_Depth8_ColSample\"\n",
    "    },\n",
    "    # Config 2 : Beaucoup d'arbres, apprentissage lent (Precision)\n",
    "    {\n",
    "        \"n_estimators\": 500,\n",
    "        \"max_depth\": 6,\n",
    "        \"learning_rate\": 0.03,    # Tres lent et minitieux\n",
    "        \"colsample_bytree\": 0.7,\n",
    "        \"name\": \"XGB_500_SlowLearn\"\n",
    "    },\n",
    "    # Config 3 : La totale \n",
    "    # Si GPU sature, XGBoost pass sur ma  RAM syst√®me grace √† ma config\n",
    "    {\n",
    "        \"n_estimators\": 300,\n",
    "        \"max_depth\": 10,          # Tres profond\n",
    "        \"learning_rate\": 0.05,\n",
    "        \"min_child_weight\": 5,    # Regularisation pour eviter overfitting\n",
    "        \"name\": \"XGB_Deep10_Heavy\"\n",
    "    }\n",
    "]\n",
    "\n",
    "for conf in configs_heavy:\n",
    "    name = conf.pop(\"name\") # on enleve le nom pour ne pas le passer au modele\n",
    "    print(f\"\\nüî• Test Config : {name}\")\n",
    "    print(f\"   Params : {conf}\")\n",
    "    \n",
    "    t0 = time.time()\n",
    "    try:\n",
    "        clf = xgb.XGBClassifier(\n",
    "            objective='multi:softmax',\n",
    "            num_class=NUM_CLASSES,\n",
    "            tree_method='hist',\n",
    "            device='cuda',        # On tente le GPU\n",
    "            **conf\n",
    "        )\n",
    "        \n",
    "        # verbose=50 pour voir si ca avance\n",
    "        clf.fit(\n",
    "            X_train, y_train,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            verbose=50\n",
    "        )\n",
    "        \n",
    "        preds = clf.predict(X_val)\n",
    "        score = f1_score(y_val_cpu, preds, average='weighted')\n",
    "        duree = time.time() - t0\n",
    "        \n",
    "        print(f\"   üíé RESULTAT {name} : F1 = {score:.4f} ({duree:.1f}s)\")\n",
    "        \n",
    "        ALL_RESULTS.append({\n",
    "            \"Famille\": \"XGBoost_Extreme\",\n",
    "            \"Details\": f\"{name} | {conf}\",\n",
    "            \"F1-Score\": score,\n",
    "            \"Temps\": duree\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è Memoire GPU satur√©e pour {name}. Passage en mode CPU (RAM 128Go)...\")\n",
    "        # Fallback sur CPU  le GPU craque \n",
    "        try:\n",
    "            clf = xgb.XGBClassifier(\n",
    "                objective='multi:softmax',\n",
    "                num_class=NUM_CLASSES,\n",
    "                tree_method='hist',\n",
    "                device='cpu',      # Mode CPU force\n",
    "                n_jobs=-1,         # Tous les coeurs\n",
    "                **conf\n",
    "            )\n",
    "            clf.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=50)\n",
    "            preds = clf.predict(X_val)\n",
    "            score = f1_score(y_val_cpu, preds, average='weighted')\n",
    "            print(f\"   üíæ SAUV√â PAR LE CPU : F1 = {score:.4f}\")\n",
    "            \n",
    "            ALL_RESULTS.append({\n",
    "                \"Famille\": \"XGBoost_Extreme_CPU\",\n",
    "                \"Details\": f\"{name}\",\n",
    "                \"F1-Score\": score,\n",
    "                \"Temps\": time.time() - t0\n",
    "            })\n",
    "        except Exception as e2:\n",
    "            print(f\"   ‚ùå Echec total : {e2}\")\n",
    "\n",
    "print(\"\\n‚úÖ Tests Extreme finis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ba2ad13-0735-44b6-9e8b-9e1ce53eba69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöú LANCEMENT XGBOOST 'HEAVY' SUR CPU (RAM 128GO)...\n",
      "   Parametres : {'n_estimators': 300, 'max_depth': 10, 'learning_rate': 0.05, 'min_child_weight': 5, 'colsample_bytree': 0.7}\n",
      "   ‚è≥ Initialisation... (peut prendre 1-2 min avant le premier affichage)\n",
      "[0]\tvalidation_0-mlogloss:3.13287\n",
      "[1]\tvalidation_0-mlogloss:3.00850\n",
      "[2]\tvalidation_0-mlogloss:2.90567\n",
      "[3]\tvalidation_0-mlogloss:2.81775\n",
      "[4]\tvalidation_0-mlogloss:2.73967\n",
      "[5]\tvalidation_0-mlogloss:2.66967\n",
      "[6]\tvalidation_0-mlogloss:2.60604\n",
      "[7]\tvalidation_0-mlogloss:2.54778\n",
      "[8]\tvalidation_0-mlogloss:2.49369\n",
      "[9]\tvalidation_0-mlogloss:2.44368\n",
      "[10]\tvalidation_0-mlogloss:2.39637\n",
      "[11]\tvalidation_0-mlogloss:2.35267\n",
      "[12]\tvalidation_0-mlogloss:2.31092\n",
      "[13]\tvalidation_0-mlogloss:2.27200\n",
      "[14]\tvalidation_0-mlogloss:2.23504\n",
      "[15]\tvalidation_0-mlogloss:2.19975\n",
      "[16]\tvalidation_0-mlogloss:2.16640\n",
      "[17]\tvalidation_0-mlogloss:2.13433\n",
      "[18]\tvalidation_0-mlogloss:2.10364\n",
      "[19]\tvalidation_0-mlogloss:2.07467\n",
      "[20]\tvalidation_0-mlogloss:2.04666\n",
      "[21]\tvalidation_0-mlogloss:2.01970\n",
      "[22]\tvalidation_0-mlogloss:1.99415\n",
      "[23]\tvalidation_0-mlogloss:1.96909\n",
      "[24]\tvalidation_0-mlogloss:1.94497\n",
      "[25]\tvalidation_0-mlogloss:1.92200\n",
      "[26]\tvalidation_0-mlogloss:1.90010\n",
      "[27]\tvalidation_0-mlogloss:1.87868\n",
      "[28]\tvalidation_0-mlogloss:1.85793\n",
      "[29]\tvalidation_0-mlogloss:1.83815\n",
      "[30]\tvalidation_0-mlogloss:1.81883\n",
      "[31]\tvalidation_0-mlogloss:1.80004\n",
      "[32]\tvalidation_0-mlogloss:1.78180\n",
      "[33]\tvalidation_0-mlogloss:1.76451\n",
      "[34]\tvalidation_0-mlogloss:1.74780\n",
      "[35]\tvalidation_0-mlogloss:1.73156\n",
      "[36]\tvalidation_0-mlogloss:1.71559\n",
      "[37]\tvalidation_0-mlogloss:1.70011\n",
      "[38]\tvalidation_0-mlogloss:1.68495\n",
      "[39]\tvalidation_0-mlogloss:1.67013\n",
      "[40]\tvalidation_0-mlogloss:1.65580\n",
      "[41]\tvalidation_0-mlogloss:1.64185\n",
      "[42]\tvalidation_0-mlogloss:1.62826\n",
      "[43]\tvalidation_0-mlogloss:1.61500\n",
      "[44]\tvalidation_0-mlogloss:1.60217\n",
      "[45]\tvalidation_0-mlogloss:1.58950\n",
      "[46]\tvalidation_0-mlogloss:1.57745\n",
      "[47]\tvalidation_0-mlogloss:1.56562\n",
      "[48]\tvalidation_0-mlogloss:1.55397\n",
      "[49]\tvalidation_0-mlogloss:1.54284\n",
      "[50]\tvalidation_0-mlogloss:1.53167\n",
      "[51]\tvalidation_0-mlogloss:1.52091\n",
      "[52]\tvalidation_0-mlogloss:1.51049\n",
      "[53]\tvalidation_0-mlogloss:1.50032\n",
      "[54]\tvalidation_0-mlogloss:1.49034\n",
      "[55]\tvalidation_0-mlogloss:1.48055\n",
      "[56]\tvalidation_0-mlogloss:1.47100\n",
      "[57]\tvalidation_0-mlogloss:1.46172\n",
      "[58]\tvalidation_0-mlogloss:1.45258\n",
      "[59]\tvalidation_0-mlogloss:1.44372\n",
      "[60]\tvalidation_0-mlogloss:1.43496\n",
      "[61]\tvalidation_0-mlogloss:1.42649\n",
      "[62]\tvalidation_0-mlogloss:1.41824\n",
      "[63]\tvalidation_0-mlogloss:1.41011\n",
      "[64]\tvalidation_0-mlogloss:1.40222\n",
      "[65]\tvalidation_0-mlogloss:1.39442\n",
      "[66]\tvalidation_0-mlogloss:1.38679\n",
      "[67]\tvalidation_0-mlogloss:1.37933\n",
      "[68]\tvalidation_0-mlogloss:1.37218\n",
      "[69]\tvalidation_0-mlogloss:1.36507\n",
      "[70]\tvalidation_0-mlogloss:1.35808\n",
      "[71]\tvalidation_0-mlogloss:1.35121\n",
      "[72]\tvalidation_0-mlogloss:1.34452\n",
      "[73]\tvalidation_0-mlogloss:1.33786\n",
      "[74]\tvalidation_0-mlogloss:1.33142\n",
      "[75]\tvalidation_0-mlogloss:1.32503\n",
      "[76]\tvalidation_0-mlogloss:1.31896\n",
      "[77]\tvalidation_0-mlogloss:1.31264\n",
      "[78]\tvalidation_0-mlogloss:1.30669\n",
      "[79]\tvalidation_0-mlogloss:1.30084\n",
      "[80]\tvalidation_0-mlogloss:1.29517\n",
      "[81]\tvalidation_0-mlogloss:1.28957\n",
      "[82]\tvalidation_0-mlogloss:1.28410\n",
      "[83]\tvalidation_0-mlogloss:1.27879\n",
      "[84]\tvalidation_0-mlogloss:1.27349\n",
      "[85]\tvalidation_0-mlogloss:1.26805\n",
      "[86]\tvalidation_0-mlogloss:1.26295\n",
      "[87]\tvalidation_0-mlogloss:1.25795\n",
      "[88]\tvalidation_0-mlogloss:1.25307\n",
      "[89]\tvalidation_0-mlogloss:1.24819\n",
      "[90]\tvalidation_0-mlogloss:1.24334\n",
      "[91]\tvalidation_0-mlogloss:1.23863\n",
      "[92]\tvalidation_0-mlogloss:1.23401\n",
      "[93]\tvalidation_0-mlogloss:1.22951\n",
      "[94]\tvalidation_0-mlogloss:1.22489\n",
      "[95]\tvalidation_0-mlogloss:1.22035\n",
      "[96]\tvalidation_0-mlogloss:1.21597\n",
      "[97]\tvalidation_0-mlogloss:1.21155\n",
      "[98]\tvalidation_0-mlogloss:1.20726\n",
      "[99]\tvalidation_0-mlogloss:1.20317\n",
      "[100]\tvalidation_0-mlogloss:1.19903\n",
      "[101]\tvalidation_0-mlogloss:1.19508\n",
      "[102]\tvalidation_0-mlogloss:1.19120\n",
      "[103]\tvalidation_0-mlogloss:1.18718\n",
      "[104]\tvalidation_0-mlogloss:1.18325\n",
      "[105]\tvalidation_0-mlogloss:1.17949\n",
      "[106]\tvalidation_0-mlogloss:1.17569\n",
      "[107]\tvalidation_0-mlogloss:1.17184\n",
      "[108]\tvalidation_0-mlogloss:1.16832\n",
      "[109]\tvalidation_0-mlogloss:1.16476\n",
      "[110]\tvalidation_0-mlogloss:1.16116\n",
      "[111]\tvalidation_0-mlogloss:1.15760\n",
      "[112]\tvalidation_0-mlogloss:1.15417\n",
      "[113]\tvalidation_0-mlogloss:1.15093\n",
      "[114]\tvalidation_0-mlogloss:1.14750\n",
      "[115]\tvalidation_0-mlogloss:1.14415\n",
      "[116]\tvalidation_0-mlogloss:1.14088\n",
      "[117]\tvalidation_0-mlogloss:1.13761\n",
      "[118]\tvalidation_0-mlogloss:1.13449\n",
      "[119]\tvalidation_0-mlogloss:1.13150\n",
      "[120]\tvalidation_0-mlogloss:1.12843\n",
      "[121]\tvalidation_0-mlogloss:1.12541\n",
      "[122]\tvalidation_0-mlogloss:1.12254\n",
      "[123]\tvalidation_0-mlogloss:1.11963\n",
      "[124]\tvalidation_0-mlogloss:1.11671\n",
      "[125]\tvalidation_0-mlogloss:1.11384\n",
      "[126]\tvalidation_0-mlogloss:1.11104\n",
      "[127]\tvalidation_0-mlogloss:1.10829\n",
      "[128]\tvalidation_0-mlogloss:1.10553\n",
      "[129]\tvalidation_0-mlogloss:1.10286\n",
      "[130]\tvalidation_0-mlogloss:1.10013\n",
      "[131]\tvalidation_0-mlogloss:1.09747\n",
      "[132]\tvalidation_0-mlogloss:1.09481\n",
      "[133]\tvalidation_0-mlogloss:1.09226\n",
      "[134]\tvalidation_0-mlogloss:1.08952\n",
      "[135]\tvalidation_0-mlogloss:1.08705\n",
      "[136]\tvalidation_0-mlogloss:1.08461\n",
      "[137]\tvalidation_0-mlogloss:1.08219\n",
      "[138]\tvalidation_0-mlogloss:1.07976\n",
      "[139]\tvalidation_0-mlogloss:1.07721\n",
      "[140]\tvalidation_0-mlogloss:1.07484\n",
      "[141]\tvalidation_0-mlogloss:1.07244\n",
      "[142]\tvalidation_0-mlogloss:1.07006\n",
      "[143]\tvalidation_0-mlogloss:1.06777\n",
      "[144]\tvalidation_0-mlogloss:1.06539\n",
      "[145]\tvalidation_0-mlogloss:1.06299\n",
      "[146]\tvalidation_0-mlogloss:1.06071\n",
      "[147]\tvalidation_0-mlogloss:1.05840\n",
      "[148]\tvalidation_0-mlogloss:1.05630\n",
      "[149]\tvalidation_0-mlogloss:1.05396\n",
      "[150]\tvalidation_0-mlogloss:1.05184\n",
      "[151]\tvalidation_0-mlogloss:1.04967\n",
      "[152]\tvalidation_0-mlogloss:1.04752\n",
      "[153]\tvalidation_0-mlogloss:1.04527\n",
      "[154]\tvalidation_0-mlogloss:1.04319\n",
      "[155]\tvalidation_0-mlogloss:1.04112\n",
      "[156]\tvalidation_0-mlogloss:1.03909\n",
      "[157]\tvalidation_0-mlogloss:1.03713\n",
      "[158]\tvalidation_0-mlogloss:1.03510\n",
      "[159]\tvalidation_0-mlogloss:1.03325\n",
      "[160]\tvalidation_0-mlogloss:1.03119\n",
      "[161]\tvalidation_0-mlogloss:1.02929\n",
      "[162]\tvalidation_0-mlogloss:1.02715\n",
      "[163]\tvalidation_0-mlogloss:1.02516\n",
      "[164]\tvalidation_0-mlogloss:1.02341\n",
      "[165]\tvalidation_0-mlogloss:1.02145\n",
      "[166]\tvalidation_0-mlogloss:1.01959\n",
      "[167]\tvalidation_0-mlogloss:1.01769\n",
      "[168]\tvalidation_0-mlogloss:1.01592\n",
      "[169]\tvalidation_0-mlogloss:1.01410\n",
      "[170]\tvalidation_0-mlogloss:1.01227\n",
      "[171]\tvalidation_0-mlogloss:1.01055\n",
      "[172]\tvalidation_0-mlogloss:1.00872\n",
      "[173]\tvalidation_0-mlogloss:1.00706\n",
      "[174]\tvalidation_0-mlogloss:1.00518\n",
      "[175]\tvalidation_0-mlogloss:1.00339\n",
      "[176]\tvalidation_0-mlogloss:1.00154\n",
      "[177]\tvalidation_0-mlogloss:0.99985\n",
      "[178]\tvalidation_0-mlogloss:0.99812\n",
      "[179]\tvalidation_0-mlogloss:0.99641\n",
      "[180]\tvalidation_0-mlogloss:0.99468\n",
      "[181]\tvalidation_0-mlogloss:0.99298\n",
      "[182]\tvalidation_0-mlogloss:0.99118\n",
      "[183]\tvalidation_0-mlogloss:0.98949\n",
      "[184]\tvalidation_0-mlogloss:0.98788\n",
      "[185]\tvalidation_0-mlogloss:0.98612\n",
      "[186]\tvalidation_0-mlogloss:0.98447\n",
      "[187]\tvalidation_0-mlogloss:0.98285\n",
      "[188]\tvalidation_0-mlogloss:0.98121\n",
      "[189]\tvalidation_0-mlogloss:0.97966\n",
      "[190]\tvalidation_0-mlogloss:0.97805\n",
      "[191]\tvalidation_0-mlogloss:0.97632\n",
      "[192]\tvalidation_0-mlogloss:0.97471\n",
      "[193]\tvalidation_0-mlogloss:0.97302\n",
      "[194]\tvalidation_0-mlogloss:0.97150\n",
      "[195]\tvalidation_0-mlogloss:0.96977\n",
      "[196]\tvalidation_0-mlogloss:0.96815\n",
      "[197]\tvalidation_0-mlogloss:0.96653\n",
      "[198]\tvalidation_0-mlogloss:0.96490\n",
      "[199]\tvalidation_0-mlogloss:0.96330\n",
      "[200]\tvalidation_0-mlogloss:0.96178\n",
      "[201]\tvalidation_0-mlogloss:0.96027\n",
      "[202]\tvalidation_0-mlogloss:0.95883\n",
      "[203]\tvalidation_0-mlogloss:0.95746\n",
      "[204]\tvalidation_0-mlogloss:0.95593\n",
      "[205]\tvalidation_0-mlogloss:0.95439\n",
      "[206]\tvalidation_0-mlogloss:0.95294\n",
      "[207]\tvalidation_0-mlogloss:0.95129\n",
      "[208]\tvalidation_0-mlogloss:0.94987\n",
      "[209]\tvalidation_0-mlogloss:0.94842\n",
      "[210]\tvalidation_0-mlogloss:0.94689\n",
      "[211]\tvalidation_0-mlogloss:0.94552\n",
      "[212]\tvalidation_0-mlogloss:0.94414\n",
      "[213]\tvalidation_0-mlogloss:0.94275\n",
      "[214]\tvalidation_0-mlogloss:0.94133\n",
      "[215]\tvalidation_0-mlogloss:0.94000\n",
      "[216]\tvalidation_0-mlogloss:0.93866\n",
      "[217]\tvalidation_0-mlogloss:0.93724\n",
      "[218]\tvalidation_0-mlogloss:0.93587\n",
      "[219]\tvalidation_0-mlogloss:0.93448\n",
      "[220]\tvalidation_0-mlogloss:0.93313\n",
      "[221]\tvalidation_0-mlogloss:0.93169\n",
      "[222]\tvalidation_0-mlogloss:0.93034\n",
      "[223]\tvalidation_0-mlogloss:0.92897\n",
      "[224]\tvalidation_0-mlogloss:0.92756\n",
      "[225]\tvalidation_0-mlogloss:0.92626\n",
      "[226]\tvalidation_0-mlogloss:0.92494\n",
      "[227]\tvalidation_0-mlogloss:0.92353\n",
      "[228]\tvalidation_0-mlogloss:0.92224\n",
      "[229]\tvalidation_0-mlogloss:0.92088\n",
      "[230]\tvalidation_0-mlogloss:0.91961\n",
      "[231]\tvalidation_0-mlogloss:0.91834\n",
      "[232]\tvalidation_0-mlogloss:0.91708\n",
      "[233]\tvalidation_0-mlogloss:0.91583\n",
      "[234]\tvalidation_0-mlogloss:0.91464\n",
      "[235]\tvalidation_0-mlogloss:0.91336\n",
      "[236]\tvalidation_0-mlogloss:0.91212\n",
      "[237]\tvalidation_0-mlogloss:0.91085\n",
      "[238]\tvalidation_0-mlogloss:0.90959\n",
      "[239]\tvalidation_0-mlogloss:0.90828\n",
      "[240]\tvalidation_0-mlogloss:0.90705\n",
      "[241]\tvalidation_0-mlogloss:0.90579\n",
      "[242]\tvalidation_0-mlogloss:0.90464\n",
      "[243]\tvalidation_0-mlogloss:0.90340\n",
      "[244]\tvalidation_0-mlogloss:0.90229\n",
      "[245]\tvalidation_0-mlogloss:0.90107\n",
      "[246]\tvalidation_0-mlogloss:0.90007\n",
      "[247]\tvalidation_0-mlogloss:0.89895\n",
      "[248]\tvalidation_0-mlogloss:0.89772\n",
      "[249]\tvalidation_0-mlogloss:0.89649\n",
      "[250]\tvalidation_0-mlogloss:0.89535\n",
      "[251]\tvalidation_0-mlogloss:0.89417\n",
      "[252]\tvalidation_0-mlogloss:0.89307\n",
      "[253]\tvalidation_0-mlogloss:0.89182\n",
      "[254]\tvalidation_0-mlogloss:0.89063\n",
      "[255]\tvalidation_0-mlogloss:0.88948\n",
      "[256]\tvalidation_0-mlogloss:0.88836\n",
      "[257]\tvalidation_0-mlogloss:0.88717\n",
      "[258]\tvalidation_0-mlogloss:0.88605\n",
      "[259]\tvalidation_0-mlogloss:0.88496\n",
      "[260]\tvalidation_0-mlogloss:0.88393\n",
      "[261]\tvalidation_0-mlogloss:0.88277\n",
      "[262]\tvalidation_0-mlogloss:0.88167\n",
      "[263]\tvalidation_0-mlogloss:0.88057\n",
      "[264]\tvalidation_0-mlogloss:0.87946\n",
      "[265]\tvalidation_0-mlogloss:0.87835\n",
      "[266]\tvalidation_0-mlogloss:0.87737\n",
      "[267]\tvalidation_0-mlogloss:0.87631\n",
      "[268]\tvalidation_0-mlogloss:0.87517\n",
      "[269]\tvalidation_0-mlogloss:0.87416\n",
      "[270]\tvalidation_0-mlogloss:0.87307\n",
      "[271]\tvalidation_0-mlogloss:0.87204\n",
      "[272]\tvalidation_0-mlogloss:0.87088\n",
      "[273]\tvalidation_0-mlogloss:0.86973\n",
      "[274]\tvalidation_0-mlogloss:0.86866\n",
      "[275]\tvalidation_0-mlogloss:0.86767\n",
      "[276]\tvalidation_0-mlogloss:0.86664\n",
      "[277]\tvalidation_0-mlogloss:0.86555\n",
      "[278]\tvalidation_0-mlogloss:0.86456\n",
      "[279]\tvalidation_0-mlogloss:0.86363\n",
      "[280]\tvalidation_0-mlogloss:0.86264\n",
      "[281]\tvalidation_0-mlogloss:0.86161\n",
      "[282]\tvalidation_0-mlogloss:0.86055\n",
      "[283]\tvalidation_0-mlogloss:0.85957\n",
      "[284]\tvalidation_0-mlogloss:0.85867\n",
      "[285]\tvalidation_0-mlogloss:0.85767\n",
      "[286]\tvalidation_0-mlogloss:0.85654\n",
      "[287]\tvalidation_0-mlogloss:0.85548\n",
      "[288]\tvalidation_0-mlogloss:0.85449\n",
      "[289]\tvalidation_0-mlogloss:0.85358\n",
      "[290]\tvalidation_0-mlogloss:0.85250\n",
      "[291]\tvalidation_0-mlogloss:0.85143\n",
      "[292]\tvalidation_0-mlogloss:0.85051\n",
      "[293]\tvalidation_0-mlogloss:0.84940\n",
      "[294]\tvalidation_0-mlogloss:0.84848\n",
      "[295]\tvalidation_0-mlogloss:0.84753\n",
      "[296]\tvalidation_0-mlogloss:0.84663\n",
      "[297]\tvalidation_0-mlogloss:0.84568\n",
      "[298]\tvalidation_0-mlogloss:0.84483\n",
      "[299]\tvalidation_0-mlogloss:0.84394\n",
      "\n",
      "‚úÖ SUCC√àS CPU : F1 = 0.7651 (21596.6s)\n",
      "\n",
      "üèÅ Test termin√©. Tu peux maintenant lancer la SYNTHESE.\n"
     ]
    }
   ],
   "source": [
    "# -XGBOOST ULTIME (MODE CPU / 128GB RAM) \n",
    "# On active un log ultra-d√©taill√© pour voir chaque arbre se construire.\n",
    "\n",
    "print(\"\\nüöú LANCEMENT XGBOOST 'HEAVY' SUR CPU (RAM 128GO)...\")\n",
    "\n",
    "# Config LOURDE (celle qui a plant√© sur GPU)\n",
    "conf_heavy = {\n",
    "    \"n_estimators\": 300,\n",
    "    \"max_depth\": 10,           # Tres profond = grosse memoire\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"min_child_weight\": 5,\n",
    "    \"colsample_bytree\": 0.7,   # On ajoute √ßa aussi pour la robustesse\n",
    "}\n",
    "\n",
    "print(f\"   Parametres : {conf_heavy}\")\n",
    "print(\"   ‚è≥ Initialisation... (peut prendre 1-2 min avant le premier affichage)\")\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "try:\n",
    "    # Force explicite device='cpu' et n_jobs=-1 (tous les coeurs)\n",
    "    clf = xgb.XGBClassifier(\n",
    "        objective='multi:softmax',\n",
    "        num_class=NUM_CLASSES,\n",
    "        tree_method='hist',   # Le plus rapide sur CPU\n",
    "        device='cpu',         # ON FORCE LE CPU\n",
    "        n_jobs=-1,            # TOUTE LA PUISSANCE DU PROCESSEUR\n",
    "        **conf_heavy\n",
    "    )\n",
    "    \n",
    "    # verbose=1 : Tu verras une ligne pour CHAQUE arbre (0, 1, 2, ... 299)\n",
    "    # Impossible de douter si √ßa avance ou pas !\n",
    "    clf.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        verbose=1 \n",
    "    )\n",
    "    \n",
    "    preds = clf.predict(X_val)\n",
    "    score = f1_score(y_val_cpu, preds, average='weighted')\n",
    "    duree = time.time() - t0\n",
    "    \n",
    "    print(f\"\\n‚úÖ SUCC√àS CPU : F1 = {score:.4f} ({duree:.1f}s)\")\n",
    "    \n",
    "    # On ajoute au tableau final\n",
    "    ALL_RESULTS.append({\n",
    "        \"Famille\": \"XGBoost_Heavy_CPU\",\n",
    "        \"Details\": \"Depth:10 | Est:300 | RAM:128Go\",\n",
    "        \"F1-Score\": score,\n",
    "        \"Temps\": duree\n",
    "    })\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Erreur : {e}\")\n",
    "\n",
    "print(\"\\nüèÅ Test termin√©. Tu peux maintenant lancer la SYNTHESE.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c5fccac-cf7e-4eb9-a7bf-bff45495e4b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lightgbm in c:\\users\\amisf\\anaconda3\\envs\\masterclass_tooling\\lib\\site-packages (4.6.0)\n",
      "Requirement already satisfied: catboost in c:\\users\\amisf\\anaconda3\\envs\\masterclass_tooling\\lib\\site-packages (1.2.8)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\amisf\\anaconda3\\envs\\masterclass_tooling\\lib\\site-packages (from lightgbm) (2.2.6)\n",
      "Requirement already satisfied: scipy in c:\\users\\amisf\\anaconda3\\envs\\masterclass_tooling\\lib\\site-packages (from lightgbm) (1.16.3)\n",
      "Requirement already satisfied: graphviz in c:\\users\\amisf\\anaconda3\\envs\\masterclass_tooling\\lib\\site-packages (from catboost) (0.21)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\amisf\\anaconda3\\envs\\masterclass_tooling\\lib\\site-packages (from catboost) (3.10.7)\n",
      "Requirement already satisfied: pandas>=0.24 in c:\\users\\amisf\\anaconda3\\envs\\masterclass_tooling\\lib\\site-packages (from catboost) (2.3.3)\n",
      "Requirement already satisfied: plotly in c:\\users\\amisf\\anaconda3\\envs\\masterclass_tooling\\lib\\site-packages (from catboost) (6.5.2)\n",
      "Requirement already satisfied: six in c:\\users\\amisf\\anaconda3\\envs\\masterclass_tooling\\lib\\site-packages (from catboost) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\amisf\\anaconda3\\envs\\masterclass_tooling\\lib\\site-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\amisf\\anaconda3\\envs\\masterclass_tooling\\lib\\site-packages (from pandas>=0.24->catboost) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\amisf\\anaconda3\\envs\\masterclass_tooling\\lib\\site-packages (from pandas>=0.24->catboost) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\amisf\\anaconda3\\envs\\masterclass_tooling\\lib\\site-packages (from matplotlib->catboost) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\amisf\\anaconda3\\envs\\masterclass_tooling\\lib\\site-packages (from matplotlib->catboost) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\amisf\\anaconda3\\envs\\masterclass_tooling\\lib\\site-packages (from matplotlib->catboost) (4.61.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\amisf\\anaconda3\\envs\\masterclass_tooling\\lib\\site-packages (from matplotlib->catboost) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\amisf\\anaconda3\\envs\\masterclass_tooling\\lib\\site-packages (from matplotlib->catboost) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\amisf\\anaconda3\\envs\\masterclass_tooling\\lib\\site-packages (from matplotlib->catboost) (12.0.0)\n",
      "Requirement already satisfied: pyparsing>=3 in c:\\users\\amisf\\anaconda3\\envs\\masterclass_tooling\\lib\\site-packages (from matplotlib->catboost) (3.2.5)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in c:\\users\\amisf\\anaconda3\\envs\\masterclass_tooling\\lib\\site-packages (from plotly->catboost) (2.15.0)\n",
      "\n",
      "üöÄ BONUS : Test de nouveaux champions GPU...\n",
      "\n",
      "üí° Test LightGBM (GPU)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amisf\\anaconda3\\envs\\masterclass_tooling\\Lib\\site-packages\\sklearn\\utils\\validation.py:2691: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> F1: 0.6958 (393.1s)\n",
      "\n",
      "üê± Test CatBoost (GPU)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: less than 75% GPU memory available for training. Free: 5745 Total: 12281.375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 3.1640266\ttotal: 661ms\tremaining: 1m 38s\n",
      "50:\tlearn: 2.0870322\ttotal: 15s\tremaining: 29.1s\n",
      "100:\tlearn: 1.8603493\ttotal: 28.7s\tremaining: 13.9s\n",
      "149:\tlearn: 1.7353702\ttotal: 42.1s\tremaining: 0us\n",
      "   -> F1: 0.4824 (49.5s)\n",
      "\n",
      "‚úÖ les mod√®les compl√©mentaires sont termin√©s\n"
     ]
    }
   ],
   "source": [
    "# MODELES ALTERNATIFS GPU \n",
    "\n",
    "# install si besoin\n",
    "import sys\n",
    "!{sys.executable} -m pip install lightgbm catboost\n",
    "\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "print(\"\\nüöÄ BONUS : Test de nouveaux champions GPU...\")\n",
    "\n",
    "# 1. LightGBM (GPU)\n",
    "print(\"\\nüí° Test LightGBM (GPU)...\")\n",
    "try:\n",
    "    t0 = time.time()\n",
    "    lgbm = lgb.LGBMClassifier(\n",
    "        device='gpu',           # acceleration gpu\n",
    "        n_estimators=150,\n",
    "        learning_rate=0.1,\n",
    "        num_leaves=31,\n",
    "        verbose=-1              \n",
    "    )\n",
    "    lgbm.fit(X_train, y_train)\n",
    "    p_lgb = lgbm.predict(X_val)\n",
    "    \n",
    "    score_lgb = f1_score(y_val_cpu, p_lgb, average='weighted')\n",
    "    duree = time.time() - t0\n",
    "    \n",
    "    print(f\"   -> F1: {score_lgb:.4f} ({duree:.1f}s)\")\n",
    "    ALL_RESULTS.append({\n",
    "        \"Famille\": \"LightGBM\",\n",
    "        \"Details\": \"GPU | N=150 | Leaves=31\",\n",
    "        \"F1-Score\": score_lgb,\n",
    "        \"Temps\": duree\n",
    "    })\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Erreur LightGBM: {e}\")\n",
    "\n",
    "# 2. CatBoost (GPU)\n",
    "print(\"\\nüê± Test CatBoost (GPU)...\")\n",
    "try:\n",
    "    t0 = time.time()\n",
    "    # Catboost est tres fort par defaut\n",
    "    cat = CatBoostClassifier(\n",
    "        task_type=\"GPU\",       # acceleration gpu\n",
    "        iterations=150,\n",
    "        learning_rate=0.1,\n",
    "        depth=6,\n",
    "        verbose=50             # log tous les 50 arbres\n",
    "    )\n",
    "    cat.fit(X_train, y_train)\n",
    "    p_cat = cat.predict(X_val)\n",
    "    \n",
    "    # catboost renvoie parfois des arrays 2D, on aplatit\n",
    "    p_cat = p_cat.flatten()\n",
    "    \n",
    "    score_cat = f1_score(y_val_cpu, p_cat, average='weighted')\n",
    "    duree = time.time() - t0\n",
    "    \n",
    "    print(f\"   -> F1: {score_cat:.4f} ({duree:.1f}s)\")\n",
    "    ALL_RESULTS.append({\n",
    "        \"Famille\": \"CatBoost\",\n",
    "        \"Details\": \"GPU | N=150 | Depth=6\",\n",
    "        \"F1-Score\": score_cat,\n",
    "        \"Temps\": duree\n",
    "    })\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Erreur CatBoost: {e}\")\n",
    "\n",
    "print(\"\\n‚úÖ les mod√®les compl√©mentaires sont termin√©s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0423ee03-191d-4f04-96ac-f8040f0062c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üå≤ lancements modeles cpu...\n",
      "\n",
      "... Random Forest (100 arbres)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 16 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 14 of 100\n",
      "building tree 1 of 100\n",
      "building tree 15 of 100\n",
      "building tree 16 of 100\n",
      "building tree 10 of 100\n",
      "building tree 12 of 100\n",
      "building tree 13 of 100\n",
      "building tree 6 of 100\n",
      "building tree 8 of 100\n",
      "building tree 3 of 100\n",
      "building tree 2 of 100\n",
      "building tree 11 of 100\n",
      "building tree 4 of 100\n",
      "building tree 5 of 100\n",
      "building tree 7 of 100\n",
      "building tree 9 of 100\n",
      "building tree 17 of 100\n",
      "building tree 18 of 100\n",
      "building tree 19 of 100\n",
      "building tree 20 of 100\n",
      "building tree 21 of 100\n",
      "building tree 22 of 100\n",
      "building tree 23 of 100\n",
      "building tree 24 of 100\n",
      "building tree 25 of 100\n",
      "building tree 26 of 100\n",
      "building tree 27 of 100\n",
      "building tree 28 of 100\n",
      "building tree 29 of 100\n",
      "building tree 30 of 100\n",
      "building tree 31 of 100\n",
      "building tree 32 of 100\n",
      "building tree 33 of 100\n",
      "building tree 34 of 100\n",
      "building tree 35 of 100\n",
      "building tree 36 of 100\n",
      "building tree 37 of 100\n",
      "building tree 38 of 100\n",
      "building tree 39 of 100\n",
      "building tree 40 of 100\n",
      "building tree 41 of 100\n",
      "building tree 42 of 100\n",
      "building tree 43 of 100\n",
      "building tree 44 of 100\n",
      "building tree 45 of 100\n",
      "building tree 46 of 100\n",
      "building tree 47 of 100\n",
      "building tree 48 of 100\n",
      "building tree 49 of 100\n",
      "building tree 50 of 100\n",
      "building tree 51 of 100\n",
      "building tree 52 of 100\n",
      "building tree 53 of 100\n",
      "building tree 54 of 100\n",
      "building tree 55 of 100\n",
      "building tree 56 of 100\n",
      "building tree 57 of 100\n",
      "building tree 58 of 100\n",
      "building tree 59 of 100\n",
      "building tree 60 of 100\n",
      "building tree 61 of 100\n",
      "building tree 62 of 100\n",
      "building tree 63 of 100\n",
      "building tree 64 of 100\n",
      "building tree 65 of 100\n",
      "building tree 66 of 100\n",
      "building tree 67 of 100\n",
      "building tree 68 of 100\n",
      "building tree 69 of 100\n",
      "building tree 70 of 100\n",
      "building tree 71 of 100\n",
      "building tree 72 of 100\n",
      "building tree 73 of 100\n",
      "building tree 74 of 100\n",
      "building tree 75 of 100\n",
      "building tree 76 of 100\n",
      "building tree 77 of 100\n",
      "building tree 78 of 100\n",
      "building tree 79 of 100\n",
      "building tree 80 of 100\n",
      "building tree 81 of 100\n",
      "building tree 82 of 100\n",
      "building tree 83 of 100\n",
      "building tree 84 of 100\n",
      "building tree 85 of 100\n",
      "building tree 86 of 100\n",
      "building tree 87 of 100\n",
      "building tree 88 of 100\n",
      "building tree 89 of 100\n",
      "building tree 90 of 100\n",
      "building tree 91 of 100\n",
      "building tree 92 of 100\n",
      "building tree 93 of 100\n",
      "building tree 94 of 100\n",
      "building tree 95 of 100\n",
      "building tree 96 of 100\n",
      "building tree 97 of 100\n",
      "building tree 98 of 100\n",
      "building tree 99 of 100\n",
      "building tree 100 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:  5.1min finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done 100 out of 100 | elapsed:    1.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "... Random Forest (200 arbres, entropy)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 16 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 16 of 200\n",
      "building tree 3 of 200\n",
      "building tree 5 of 200\n",
      "building tree 7 of 200\n",
      "building tree 8 of 200\n",
      "building tree 9 of 200\n",
      "building tree 10 of 200\n",
      "building tree 11 of 200\n",
      "building tree 12 of 200\n",
      "building tree 13 of 200\n",
      "building tree 15 of 200\n",
      "building tree 14 of 200\n",
      "building tree 1 of 200\n",
      "building tree 4 of 200\n",
      "building tree 6 of 200\n",
      "building tree 2 of 200\n",
      "building tree 17 of 200\n",
      "building tree 18 of 200\n",
      "building tree 19 of 200\n",
      "building tree 20 of 200\n",
      "building tree 21 of 200\n",
      "building tree 22 of 200\n",
      "building tree 23 of 200\n",
      "building tree 24 of 200\n",
      "building tree 25 of 200\n",
      "building tree 26 of 200\n",
      "building tree 27 of 200\n",
      "building tree 28 of 200\n",
      "building tree 29 of 200\n",
      "building tree 30 of 200\n",
      "building tree 31 of 200\n",
      "building tree 32 of 200\n",
      "building tree 33 of 200\n",
      "building tree 34 of 200\n",
      "building tree 35 of 200\n",
      "building tree 36 of 200\n",
      "building tree 37 of 200\n",
      "building tree 38 of 200\n",
      "building tree 39 of 200\n",
      "building tree 40 of 200\n",
      "building tree 41 of 200\n",
      "building tree 42 of 200\n",
      "building tree 43 of 200\n",
      "building tree 44 of 200building tree 45 of 200\n",
      "\n",
      "building tree 46 of 200\n",
      "building tree 47 of 200\n",
      "building tree 48 of 200\n",
      "building tree 49 of 200\n",
      "building tree 50 of 200\n",
      "building tree 51 of 200\n",
      "building tree 52 of 200\n",
      "building tree 53 of 200\n",
      "building tree 54 of 200\n",
      "building tree 55 of 200\n",
      "building tree 56 of 200\n",
      "building tree 57 of 200\n",
      "building tree 58 of 200\n",
      "building tree 59 of 200\n",
      "building tree 60 of 200\n",
      "building tree 61 of 200\n",
      "building tree 62 of 200\n",
      "building tree 63 of 200\n",
      "building tree 64 of 200\n",
      "building tree 65 of 200\n",
      "building tree 66 of 200\n",
      "building tree 67 of 200\n",
      "building tree 68 of 200\n",
      "building tree 69 of 200\n",
      "building tree 70 of 200\n",
      "building tree 71 of 200\n",
      "building tree 72 of 200\n",
      "building tree 73 of 200\n",
      "building tree 74 of 200\n",
      "building tree 75 of 200\n",
      "building tree 76 of 200\n",
      "building tree 77 of 200\n",
      "building tree 78 of 200\n",
      "building tree 79 of 200\n",
      "building tree 80 of 200\n",
      "building tree 81 of 200\n",
      "building tree 82 of 200\n",
      "building tree 83 of 200\n",
      "building tree 84 of 200\n",
      "building tree 85 of 200\n",
      "building tree 86 of 200\n",
      "building tree 87 of 200\n",
      "building tree 88 of 200\n",
      "building tree 89 of 200\n",
      "building tree 90 of 200\n",
      "building tree 91 of 200\n",
      "building tree 92 of 200\n",
      "building tree 93 of 200\n",
      "building tree 94 of 200\n",
      "building tree 95 of 200\n",
      "building tree 96 of 200\n",
      "building tree 97 of 200\n",
      "building tree 98 of 200\n",
      "building tree 99 of 200\n",
      "building tree 100 of 200\n",
      "building tree 101 of 200\n",
      "building tree 102 of 200\n",
      "building tree 103 of 200\n",
      "building tree 104 of 200\n",
      "building tree 105 of 200\n",
      "building tree 106 of 200\n",
      "building tree 107 of 200\n",
      "building tree 108 of 200\n",
      "building tree 109 of 200\n",
      "building tree 110 of 200\n",
      "building tree 111 of 200\n",
      "building tree 112 of 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  96 tasks      | elapsed: 15.4min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 113 of 200\n",
      "building tree 114 of 200\n",
      "building tree 115 of 200\n",
      "building tree 116 of 200\n",
      "building tree 117 of 200\n",
      "building tree 118 of 200\n",
      "building tree 119 of 200\n",
      "building tree 120 of 200\n",
      "building tree 121 of 200\n",
      "building tree 122 of 200\n",
      "building tree 123 of 200\n",
      "building tree 124 of 200\n",
      "building tree 125 of 200\n",
      "building tree 126 of 200\n",
      "building tree 127 of 200\n",
      "building tree 128 of 200\n",
      "building tree 129 of 200\n",
      "building tree 130 of 200\n",
      "building tree 131 of 200\n",
      "building tree 132 of 200\n",
      "building tree 133 of 200\n",
      "building tree 134 of 200\n",
      "building tree 135 of 200\n",
      "building tree 136 of 200\n",
      "building tree 137 of 200\n",
      "building tree 138 of 200\n",
      "building tree 139 of 200\n",
      "building tree 140 of 200\n",
      "building tree 141 of 200\n",
      "building tree 142 of 200\n",
      "building tree 143 of 200\n",
      "building tree 144 of 200\n",
      "building tree 145 of 200\n",
      "building tree 146 of 200\n",
      "building tree 147 of 200\n",
      "building tree 148 of 200\n",
      "building tree 149 of 200\n",
      "building tree 150 of 200\n",
      "building tree 151 of 200\n",
      "building tree 152 of 200\n",
      "building tree 153 of 200\n",
      "building tree 154 of 200\n",
      "building tree 155 of 200\n",
      "building tree 156 of 200\n",
      "building tree 157 of 200\n",
      "building tree 158 of 200\n",
      "building tree 159 of 200\n",
      "building tree 160 of 200\n",
      "building tree 161 of 200\n",
      "building tree 162 of 200\n",
      "building tree 163 of 200\n",
      "building tree 164 of 200\n",
      "building tree 165 of 200\n",
      "building tree 166 of 200\n",
      "building tree 167 of 200\n",
      "building tree 168 of 200\n",
      "building tree 169 of 200\n",
      "building tree 170 of 200\n",
      "building tree 171 of 200\n",
      "building tree 172 of 200\n",
      "building tree 173 of 200\n",
      "building tree 174 of 200\n",
      "building tree 175 of 200\n",
      "building tree 176 of 200\n",
      "building tree 177 of 200\n",
      "building tree 178 of 200\n",
      "building tree 179 of 200\n",
      "building tree 180 of 200\n",
      "building tree 181 of 200\n",
      "building tree 182 of 200\n",
      "building tree 183 of 200\n",
      "building tree 184 of 200\n",
      "building tree 185 of 200\n",
      "building tree 186 of 200\n",
      "building tree 187 of 200\n",
      "building tree 188 of 200\n",
      "building tree 189 of 200\n",
      "building tree 190 of 200\n",
      "building tree 191 of 200\n",
      "building tree 192 of 200\n",
      "building tree 193 of 200\n",
      "building tree 194 of 200\n",
      "building tree 195 of 200\n",
      "building tree 196 of 200\n",
      "building tree 197 of 200\n",
      "building tree 198 of 200\n",
      "building tree 199 of 200\n",
      "building tree 200 of 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed: 31.2min finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  96 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=16)]: Done 200 out of 200 | elapsed:    2.7s finished\n",
      "C:\\Users\\amisf\\anaconda3\\envs\\masterclass_tooling\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1184: FutureWarning: 'n_jobs' has no effect since 1.8 and will be removed in 1.10. You provided 'n_jobs=-1', please leave it unspecified.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "... Regression Logistique\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amisf\\anaconda3\\envs\\masterclass_tooling\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:406: ConvergenceWarning: lbfgs failed to converge after 1000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=1000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "#  MODELES CPU (AVEC LOGS D'ACTIVITE) \n",
    "print(\"\\nüå≤ lancements modeles cpu...\")\n",
    "\n",
    "# 1. Random Forest (config 1)\n",
    "print(\"\\n... Random Forest (100 arbres)\")\n",
    "t0 = time.time()\n",
    "# verbose=3 affiche l'avancement des jobs et des arbres\n",
    "rf1 = RandomForestClassifier(n_estimators=100, n_jobs=-1, verbose=3)\n",
    "rf1.fit(X_train, y_train)\n",
    "p_rf1 = rf1.predict(X_val)\n",
    "\n",
    "ALL_RESULTS.append({\n",
    "    \"Famille\": \"RandomForest\",\n",
    "    \"Details\": \"N=100 | Default\",\n",
    "    \"F1-Score\": f1_score(y_val_cpu, p_rf1, average='weighted'),\n",
    "    \"Temps\": time.time() - t0\n",
    "})\n",
    "\n",
    "# 2. Random Forest (config 2)\n",
    "print(\"\\n... Random Forest (200 arbres, entropy)\")\n",
    "t0 = time.time()\n",
    "# verbose=3 pour voir le defilement\n",
    "rf2 = RandomForestClassifier(n_estimators=200, criterion='entropy', n_jobs=-1, verbose=3)\n",
    "rf2.fit(X_train, y_train)\n",
    "p_rf2 = rf2.predict(X_val)\n",
    "\n",
    "ALL_RESULTS.append({\n",
    "    \"Famille\": \"RandomForest\",\n",
    "    \"Details\": \"N=200 | Entropy\",\n",
    "    \"F1-Score\": f1_score(y_val_cpu, p_rf2, average='weighted'),\n",
    "    \"Temps\": time.time() - t0\n",
    "})\n",
    "\n",
    "# 3. Regression Logistique\n",
    "print(\"\\n... Regression Logistique\")\n",
    "t0 = time.time()\n",
    "# verbose=1 affiche les iterations du solveur\n",
    "lr = LogisticRegression(max_iter=1000, n_jobs=-1, verbose=1)\n",
    "lr.fit(X_train, y_train)\n",
    "p_lr = lr.predict(X_val)\n",
    "\n",
    "ALL_RESULTS.append({\n",
    "    \"Famille\": \"LogisticReg\",\n",
    "    \"Details\": \"Baseline\",\n",
    "    \"F1-Score\": f1_score(y_val_cpu, p_lr, average='weighted'),\n",
    "    \"Temps\": time.time() - t0\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "57d82bfd-b80b-4052-a4e7-6c0f2ae62017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä PODIUM : MACHINE LEARNING CLASSIQUE\n",
      "                   Famille  \\\n",
      "Rang ML                      \n",
      "1        XGBoost_Heavy_CPU   \n",
      "2          XGBoost_Extreme   \n",
      "3             RandomForest   \n",
      "4                  XGBoost   \n",
      "5             RandomForest   \n",
      "6                 LightGBM   \n",
      "7                  XGBoost   \n",
      "8          XGBoost_Extreme   \n",
      "9                  XGBoost   \n",
      "10             LogisticReg   \n",
      "11                 XGBoost   \n",
      "12                 XGBoost   \n",
      "13                 XGBoost   \n",
      "14                 XGBoost   \n",
      "15                 XGBoost   \n",
      "16                CatBoost   \n",
      "\n",
      "                                                                                                     Details  \\\n",
      "Rang ML                                                                                                        \n",
      "1                                                                             Depth:10 | Est:300 | RAM:128Go   \n",
      "2        XGB_Depth8_ColSample | {'n_estimators': 300, 'max_depth': 8, 'learning_rate': 0.05, 'colsample_b...   \n",
      "3                                                                                            N=200 | Entropy   \n",
      "4                              {'n_estimators': 150, 'max_depth': 6, 'learning_rate': 0.2, 'subsample': 0.8}   \n",
      "5                                                                                            N=100 | Default   \n",
      "6                                                                                    GPU | N=150 | Leaves=31   \n",
      "7                              {'n_estimators': 100, 'max_depth': 6, 'learning_rate': 0.2, 'subsample': 0.8}   \n",
      "8        XGB_500_SlowLearn | {'n_estimators': 500, 'max_depth': 6, 'learning_rate': 0.03, 'colsample_bytr...   \n",
      "9                              {'n_estimators': 150, 'max_depth': 6, 'learning_rate': 0.1, 'subsample': 0.8}   \n",
      "10                                                                                                  Baseline   \n",
      "11                             {'n_estimators': 100, 'max_depth': 6, 'learning_rate': 0.1, 'subsample': 0.8}   \n",
      "12                             {'n_estimators': 150, 'max_depth': 4, 'learning_rate': 0.2, 'subsample': 0.8}   \n",
      "13                             {'n_estimators': 100, 'max_depth': 4, 'learning_rate': 0.2, 'subsample': 0.8}   \n",
      "14                             {'n_estimators': 150, 'max_depth': 4, 'learning_rate': 0.1, 'subsample': 0.8}   \n",
      "15                             {'n_estimators': 100, 'max_depth': 4, 'learning_rate': 0.1, 'subsample': 0.8}   \n",
      "16                                                                                     GPU | N=150 | Depth=6   \n",
      "\n",
      "         F1-Score         Temps  \n",
      "Rang ML                          \n",
      "1        0.765149  21596.595834  \n",
      "2        0.731539    999.126112  \n",
      "3        0.724154   1874.213511  \n",
      "4        0.712032    308.897644  \n",
      "5        0.710077    309.821128  \n",
      "6        0.695809    393.099905  \n",
      "7        0.675220    213.435299  \n",
      "8        0.664067    921.352189  \n",
      "9        0.663243    314.057100  \n",
      "10       0.629783   1148.110740  \n",
      "11       0.627065    219.262397  \n",
      "12       0.621142    189.144520  \n",
      "13       0.588989    137.187219  \n",
      "14       0.575696    197.923569  \n",
      "15       0.546098    142.093629  \n",
      "16       0.482448     49.473882  \n"
     ]
    }
   ],
   "source": [
    "# CELLULE A : CLASSEMENT MACHINE LEARNING \n",
    "print(\"\\nüìä PODIUM : MACHINE LEARNING CLASSIQUE\")\n",
    "\n",
    "df_all = pd.DataFrame(ALL_RESULTS)\n",
    "\n",
    "# CORRECTION DU FILTRE : On exclut aussi \"DeepLearning\"\n",
    "mask_ml = ~df_all['Famille'].astype(str).str.contains(\"PyTorch|DL|DeepLearning\", case=False, regex=True)\n",
    "df_ml = df_all[mask_ml].copy()\n",
    "\n",
    "if not df_ml.empty:\n",
    "    df_ml = df_ml.sort_values(by=\"F1-Score\", ascending=False).reset_index(drop=True)\n",
    "    df_ml.index += 1\n",
    "    df_ml.index.name = 'Rang ML'\n",
    "    \n",
    "    pd.set_option('display.max_colwidth', 100)\n",
    "    print(df_ml[['Famille', 'Details', 'F1-Score', 'Temps']])\n",
    "    \n",
    "    df_ml.to_csv(os.path.join(OUTPUT_DIR, \"classement_ml_classique.csv\"), index=True)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Aucun r√©sultat ML classique trouv√©.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bb2a7757-57aa-40f0-9a9e-c7c03b8406e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on passe au mode deep learning pour essayer \n"
     ]
    }
   ],
   "source": [
    "print(\"on passe au mode deep learning pour essayer \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c01eef41-2893-4143-875d-21b12c42d830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Importation des modules de Deep Learning (PyTorch)...\n",
      "üîÑ Passage des donn√©es en VRAM (M√©moire Vid√©o) pour acc√©l√©ration maximale...\n",
      "‚úÖ Architecture Deep Learning pr√™te sur NVIDIA GeForce RTX 4070.\n"
     ]
    }
   ],
   "source": [
    "# \"Les scores sont bons, mais peut-on aller plus loin avec les features complexes du ResNet ?\" , C'est le moment d'importer les librairies de Deep Learning.\n",
    "\n",
    "print(\"üß† Importation des modules de Deep Learning (PyTorch)...\")\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "print(\"üîÑ Passage des donn√©es en VRAM (M√©moire Vid√©o) pour acc√©l√©ration maximale...\")\n",
    "\n",
    "# Conversion explicite Numpy -> PyTorch Tensors\n",
    "X_train_t = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "y_train_t = torch.tensor(y_train, dtype=torch.long).to(device)\n",
    "X_val_t = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "\n",
    "# Cr√©ation du DataLoader (Batch Size 4096 car tu as 12Go de VRAM, on en profite)\n",
    "train_ds = TensorDataset(X_train_t, y_train_t)\n",
    "gpu_loader = DataLoader(train_ds, batch_size=4096, shuffle=True)\n",
    "\n",
    "print(f\"‚úÖ Architecture Deep Learning pr√™te sur {device_name}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1023699-59ea-4e55-911f-d4163028d2cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: torch 2.7.1+cu118\n",
      "Uninstalling torch-2.7.1+cu118:\n",
      "  Successfully uninstalled torch-2.7.1+cu118\n",
      "Found existing installation: torchvision 0.22.1+cu118\n",
      "Uninstalling torchvision-0.22.1+cu118:\n",
      "  Successfully uninstalled torchvision-0.22.1+cu118\n",
      "Found existing installation: torchaudio 2.7.1+cu118\n",
      "Uninstalling torchaudio-2.7.1+cu118:\n",
      "  Successfully uninstalled torchaudio-2.7.1+cu118\n",
      "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
      "Collecting torch\n",
      "  Using cached https://download.pytorch.org/whl/cu118/torch-2.7.1%2Bcu118-cp313-cp313-win_amd64.whl.metadata (27 kB)\n",
      "Collecting torchvision\n",
      "  Using cached https://download.pytorch.org/whl/cu118/torchvision-0.22.1%2Bcu118-cp313-cp313-win_amd64.whl.metadata (6.3 kB)\n",
      "Collecting torchaudio\n",
      "  Using cached https://download.pytorch.org/whl/cu118/torchaudio-2.7.1%2Bcu118-cp313-cp313-win_amd64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\amisf\\anaconda3\\envs\\masterclass_tooling\\lib\\site-packages (from torch) (3.20.3)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\amisf\\anaconda3\\envs\\masterclass_tooling\\lib\\site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\amisf\\anaconda3\\envs\\masterclass_tooling\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\amisf\\anaconda3\\envs\\masterclass_tooling\\lib\\site-packages (from torch) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\amisf\\anaconda3\\envs\\masterclass_tooling\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\amisf\\anaconda3\\envs\\masterclass_tooling\\lib\\site-packages (from torch) (2026.1.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\amisf\\anaconda3\\envs\\masterclass_tooling\\lib\\site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\amisf\\anaconda3\\envs\\masterclass_tooling\\lib\\site-packages (from torchvision) (2.2.6)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\amisf\\anaconda3\\envs\\masterclass_tooling\\lib\\site-packages (from torchvision) (12.0.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\amisf\\anaconda3\\envs\\masterclass_tooling\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\amisf\\anaconda3\\envs\\masterclass_tooling\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Using cached https://download.pytorch.org/whl/cu118/torch-2.7.1%2Bcu118-cp313-cp313-win_amd64.whl (2817.2 MB)\n",
      "Using cached https://download.pytorch.org/whl/cu118/torchvision-0.22.1%2Bcu118-cp313-cp313-win_amd64.whl (5.5 MB)\n",
      "Using cached https://download.pytorch.org/whl/cu118/torchaudio-2.7.1%2Bcu118-cp313-cp313-win_amd64.whl (4.1 MB)\n",
      "Installing collected packages: torch, torchvision, torchaudio\n",
      "\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ---------------------------------------- 0/3 [torch]\n",
      "   ------------- -------------------------- 1/3 [torchvision]\n",
      "   ------------- -------------------------- 1/3 [torchvision]\n",
      "   ------------- -------------------------- 1/3 [torchvision]\n",
      "   ------------- -------------------------- 1/3 [torchvision]\n",
      "   ------------- -------------------------- 1/3 [torchvision]\n",
      "   ------------- -------------------------- 1/3 [torchvision]\n",
      "   ------------- -------------------------- 1/3 [torchvision]\n",
      "   ------------- -------------------------- 1/3 [torchvision]\n",
      "   ------------- -------------------------- 1/3 [torchvision]\n",
      "   ------------- -------------------------- 1/3 [torchvision]\n",
      "   ------------- -------------------------- 1/3 [torchvision]\n",
      "   -------------------------- ------------- 2/3 [torchaudio]\n",
      "   -------------------------- ------------- 2/3 [torchaudio]\n",
      "   -------------------------- ------------- 2/3 [torchaudio]\n",
      "   -------------------------- ------------- 2/3 [torchaudio]\n",
      "   -------------------------- ------------- 2/3 [torchaudio]\n",
      "   ---------------------------------------- 3/3 [torchaudio]\n",
      "\n",
      "Successfully installed torch-2.7.1+cu118 torchaudio-2.7.1+cu118 torchvision-0.22.1+cu118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The scripts torchfrtrace.exe and torchrun.exe are installed in 'C:\\Users\\amisf\\anaconda3\\envs\\masterclass_tooling\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# d√©sinstalle la version cpu et force la version compatible CUDA (GPU NVIDIA)\n",
    "!{sys.executable} -m pip uninstall -y torch torchvision torchaudio\n",
    "!{sys.executable} -m pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "53d8ffed-9d43-4be5-8285-e99e266b6c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Transition vers le Deep Learning...\n",
      "üöÄ Envoi des donn√©es vers la m√©moire GPU (VRAM)...\n",
      "‚úÖ Tenseurs PyTorch pr√™ts dans la VRAM. Le r√©seau de neurones peut d√©marrer.\n"
     ]
    }
   ],
   "source": [
    "#  ACTIVATION MODE DEEP LEARNING (VRAM) \n",
    "print(\"üîÑ Transition vers le Deep Learning...\")\n",
    "print(\"üöÄ Envoi des donn√©es vers la m√©moire GPU (VRAM)...\")\n",
    "\n",
    "# C'est ici qu'on convertit nos tableaux Numpy en Tenseurs PyTorch\n",
    "# Et qu'on les d√©place physiquement sur la carte graphique\n",
    "X_train_t = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "y_train_t = torch.tensor(y_train, dtype=torch.long).to(device)\n",
    "X_val_t = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "\n",
    "# Cr√©ation du DataLoader PyTorch (Sp√©cifique au Deep Learning)\n",
    "# On met un gros batch_size car le GPU aime √ßa\n",
    "train_ds = TensorDataset(X_train_t, y_train_t)\n",
    "gpu_loader = DataLoader(train_ds, batch_size=4096, shuffle=True)\n",
    "\n",
    "print(\"‚úÖ Tenseurs PyTorch pr√™ts dans la VRAM. Le r√©seau de neurones peut d√©marrer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1852b093-3821-4385-a3e6-5fd8ae309d60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî• 24 configurations PyTorch a tester...\n",
      "   ... test 1/24: [1024, 512] - adam\n",
      "   ... test 2/24: [1024, 512] - rmsprop\n",
      "   ... test 3/24: [1024, 512] - adam\n",
      "   ... test 4/24: [1024, 512] - rmsprop\n",
      "   ... test 5/24: [1024, 512] - adam\n",
      "   ... test 6/24: [1024, 512] - rmsprop\n",
      "   ... test 7/24: [1024, 512] - adam\n",
      "   ... test 8/24: [1024, 512] - rmsprop\n",
      "   ... test 9/24: [2048, 1024, 512] - adam\n",
      "   ... test 10/24: [2048, 1024, 512] - rmsprop\n",
      "   ... test 11/24: [2048, 1024, 512] - adam\n",
      "   ... test 12/24: [2048, 1024, 512] - rmsprop\n",
      "   ... test 13/24: [2048, 1024, 512] - adam\n",
      "   ... test 14/24: [2048, 1024, 512] - rmsprop\n",
      "   ... test 15/24: [2048, 1024, 512] - adam\n",
      "   ... test 16/24: [2048, 1024, 512] - rmsprop\n",
      "   ... test 17/24: [512, 256] - adam\n",
      "   ... test 18/24: [512, 256] - rmsprop\n",
      "   ... test 19/24: [512, 256] - adam\n",
      "   ... test 20/24: [512, 256] - rmsprop\n",
      "   ... test 21/24: [512, 256] - adam\n",
      "   ... test 22/24: [512, 256] - rmsprop\n",
      "   ... test 23/24: [512, 256] - adam\n",
      "   ... test 24/24: [512, 256] - rmsprop\n"
     ]
    }
   ],
   "source": [
    "#  CLASSE PYTORCH DYNAMIQUE \n",
    "class ModularMLP(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_layers, dropout_rate, activation_fn):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        in_d = input_dim\n",
    "        \n",
    "        # choix activation\n",
    "        if activation_fn == 'relu': act = nn.ReLU()\n",
    "        elif activation_fn == 'gelu': act = nn.GELU()\n",
    "        elif activation_fn == 'leaky': act = nn.LeakyReLU(0.1)\n",
    "        \n",
    "        # construction couches\n",
    "        for h_dim in hidden_layers:\n",
    "            layers.append(nn.Linear(in_d, h_dim))\n",
    "            layers.append(nn.BatchNorm1d(h_dim))\n",
    "            layers.append(act)\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "            in_d = h_dim\n",
    "            \n",
    "        layers.append(nn.Linear(in_d, output_dim))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# FONCTION TEST UNITAIRE \n",
    "def run_pytorch_test(config):\n",
    "    # init model\n",
    "    model = ModularMLP(\n",
    "        INPUT_DIM, NUM_CLASSES, \n",
    "        config['layers'], config['drop'], config['act']\n",
    "    ).to(device)\n",
    "    \n",
    "    # optimizer\n",
    "    if config['opt'] == 'adam': optimizer = optim.Adam(model.parameters(), lr=config['lr'])\n",
    "    elif config['opt'] == 'sgd': optimizer = optim.SGD(model.parameters(), lr=config['lr'], momentum=0.9)\n",
    "    elif config['opt'] == 'rmsprop': optimizer = optim.RMSprop(model.parameters(), lr=config['lr'])\n",
    "    \n",
    "    crit = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # train\n",
    "    model.train()\n",
    "    t0 = time.time()\n",
    "    for ep in range(config['epochs']):\n",
    "        for bx, by in gpu_loader:\n",
    "            optimizer.zero_grad()\n",
    "            out = model(bx)\n",
    "            loss = crit(out, by)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "    duree = time.time() - t0\n",
    "    \n",
    "    # eval\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out_val = model(X_val_t)\n",
    "        _, preds = torch.max(out_val, 1)\n",
    "    \n",
    "    preds_cpu = preds.cpu().numpy()\n",
    "    score = f1_score(y_val_cpu, preds_cpu, average='weighted')\n",
    "    \n",
    "    return {\n",
    "        \"Famille\": \"DeepLearning\",\n",
    "        \"Details\": f\"L:{config['layers']} | Opt:{config['opt']} | Act:{config['act']} | Drop:{config['drop']}\",\n",
    "        \"F1-Score\": score,\n",
    "        \"Temps\": duree\n",
    "    }\n",
    "\n",
    "#  GRILLE HYPERPARAMETRES PYTORCH \n",
    "# ca va faire beaucoup de modeles !\n",
    "grid_pt = {\n",
    "    'layers': [[1024, 512], [2048, 1024, 512], [512, 256]],\n",
    "    'act': ['relu', 'gelu'],\n",
    "    'drop': [0.2, 0.5],\n",
    "    'opt': ['adam', 'rmsprop'],\n",
    "    'lr': [0.001],\n",
    "    'epochs': [20] # on fixe a 20 pr aller vite\n",
    "}\n",
    "\n",
    "# generation des combinaisons\n",
    "keys, values = zip(*grid_pt.items())\n",
    "pt_configs = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "\n",
    "print(f\"üî• {len(pt_configs)} configurations PyTorch a tester...\")\n",
    "\n",
    "# execution boucle\n",
    "for i, conf in enumerate(pt_configs):\n",
    "    print(f\"   ... test {i+1}/{len(pt_configs)}: {conf['layers']} - {conf['opt']}\")\n",
    "    res = run_pytorch_test(conf)\n",
    "    ALL_RESULTS.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "34da5665-421d-44b9-b014-6d5b8bf1910b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß† PODIUM : DEEP LEARNING (NEURAL NETWORKS) \n",
      "              Famille  \\\n",
      "Rang DL                 \n",
      "1        DeepLearning   \n",
      "2        DeepLearning   \n",
      "3        DeepLearning   \n",
      "4        DeepLearning   \n",
      "5        DeepLearning   \n",
      "6        DeepLearning   \n",
      "7        DeepLearning   \n",
      "8        DeepLearning   \n",
      "9        DeepLearning   \n",
      "10       DeepLearning   \n",
      "11       DeepLearning   \n",
      "12       DeepLearning   \n",
      "13       DeepLearning   \n",
      "14       DeepLearning   \n",
      "15       DeepLearning   \n",
      "16       DeepLearning   \n",
      "17       DeepLearning   \n",
      "18       DeepLearning   \n",
      "19       DeepLearning   \n",
      "20       DeepLearning   \n",
      "21       DeepLearning   \n",
      "22       DeepLearning   \n",
      "23       DeepLearning   \n",
      "24       DeepLearning   \n",
      "\n",
      "                                                         Details  F1-Score  \\\n",
      "Rang DL                                                                      \n",
      "1           L:[2048, 1024, 512] | Opt:adam | Act:gelu | Drop:0.2  0.914124   \n",
      "2           L:[2048, 1024, 512] | Opt:adam | Act:relu | Drop:0.2  0.909223   \n",
      "3                 L:[1024, 512] | Opt:adam | Act:gelu | Drop:0.2  0.898055   \n",
      "4           L:[2048, 1024, 512] | Opt:adam | Act:gelu | Drop:0.5  0.896616   \n",
      "5                 L:[1024, 512] | Opt:adam | Act:relu | Drop:0.2  0.894012   \n",
      "6           L:[2048, 1024, 512] | Opt:adam | Act:relu | Drop:0.5  0.878822   \n",
      "7                 L:[1024, 512] | Opt:adam | Act:gelu | Drop:0.5  0.876032   \n",
      "8                  L:[512, 256] | Opt:adam | Act:gelu | Drop:0.2  0.871171   \n",
      "9                  L:[512, 256] | Opt:adam | Act:relu | Drop:0.2  0.859674   \n",
      "10                L:[1024, 512] | Opt:adam | Act:relu | Drop:0.5  0.851672   \n",
      "11             L:[1024, 512] | Opt:rmsprop | Act:gelu | Drop:0.5  0.833182   \n",
      "12                 L:[512, 256] | Opt:adam | Act:gelu | Drop:0.5  0.821772   \n",
      "13       L:[2048, 1024, 512] | Opt:rmsprop | Act:gelu | Drop:0.5  0.821750   \n",
      "14       L:[2048, 1024, 512] | Opt:rmsprop | Act:gelu | Drop:0.2  0.817717   \n",
      "15       L:[2048, 1024, 512] | Opt:rmsprop | Act:relu | Drop:0.5  0.814265   \n",
      "16                 L:[512, 256] | Opt:adam | Act:relu | Drop:0.5  0.801429   \n",
      "17             L:[1024, 512] | Opt:rmsprop | Act:gelu | Drop:0.2  0.793542   \n",
      "18       L:[2048, 1024, 512] | Opt:rmsprop | Act:relu | Drop:0.2  0.790641   \n",
      "19             L:[1024, 512] | Opt:rmsprop | Act:relu | Drop:0.5  0.785280   \n",
      "20             L:[1024, 512] | Opt:rmsprop | Act:relu | Drop:0.2  0.763385   \n",
      "21              L:[512, 256] | Opt:rmsprop | Act:gelu | Drop:0.2  0.763060   \n",
      "22              L:[512, 256] | Opt:rmsprop | Act:gelu | Drop:0.5  0.761383   \n",
      "23              L:[512, 256] | Opt:rmsprop | Act:relu | Drop:0.5  0.754779   \n",
      "24              L:[512, 256] | Opt:rmsprop | Act:relu | Drop:0.2  0.754260   \n",
      "\n",
      "             Temps  \n",
      "Rang DL             \n",
      "1        55.746684  \n",
      "2        57.704077  \n",
      "3        58.309646  \n",
      "4        55.682217  \n",
      "5        76.741786  \n",
      "6        56.243344  \n",
      "7        56.402972  \n",
      "8        54.539268  \n",
      "9        54.979726  \n",
      "10       57.948211  \n",
      "11       56.645866  \n",
      "12       54.903606  \n",
      "13       55.703519  \n",
      "14       55.509129  \n",
      "15       56.521554  \n",
      "16       55.407437  \n",
      "17       56.680014  \n",
      "18       55.698065  \n",
      "19       56.513105  \n",
      "20       58.937616  \n",
      "21       54.712121  \n",
      "22       54.819009  \n",
      "23       54.495977  \n",
      "24       57.574389  \n"
     ]
    }
   ],
   "source": [
    "# CLASSEMENT DEEP LEARNING (PYTORCH) \n",
    "print(\"\\nüß† PODIUM : DEEP LEARNING (NEURAL NETWORKS) \")\n",
    "\n",
    "df_all = pd.DataFrame(ALL_RESULTS)\n",
    "\n",
    "# CORRECTION DU FILTRE : On garde ce qui contient \"DeepLearning\" ou \"PyTorch\"\n",
    "mask_dl = df_all['Famille'].astype(str).str.contains(\"PyTorch|DL|DeepLearning\", case=False, regex=True)\n",
    "df_dl = df_all[mask_dl].copy()\n",
    "\n",
    "if not df_dl.empty:\n",
    "    df_dl = df_dl.sort_values(by=\"F1-Score\", ascending=False).reset_index(drop=True)\n",
    "    df_dl.index += 1\n",
    "    df_dl.index.name = 'Rang DL'\n",
    "    \n",
    "    print(df_dl[['Famille', 'Details', 'F1-Score', 'Temps']])\n",
    "    df_dl.to_csv(os.path.join(OUTPUT_DIR, \"classement_deep_learning.csv\"), index=True)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Aucun r√©sultat Deep Learning trouv√©.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7d733e4c-b524-4aad-8cf8-053db4c976f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maintenant on vois tout le monde pour faire une comparaison\n"
     ]
    }
   ],
   "source": [
    "print(\"maintenant on vois tout le monde pour faire une comparaison\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "15a7c0fa-73ce-4021-95d7-f8687af8a05a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèÜ  GRAND VAINQUEUR TOUTES CATEGORIES \n",
      "                       Famille  \\\n",
      "Rang Global                      \n",
      "1                 DeepLearning   \n",
      "2                 DeepLearning   \n",
      "3                 DeepLearning   \n",
      "4                 DeepLearning   \n",
      "5                 DeepLearning   \n",
      "6                 DeepLearning   \n",
      "7                 DeepLearning   \n",
      "8                 DeepLearning   \n",
      "9                 DeepLearning   \n",
      "10                DeepLearning   \n",
      "11                DeepLearning   \n",
      "12                DeepLearning   \n",
      "13                DeepLearning   \n",
      "14                DeepLearning   \n",
      "15                DeepLearning   \n",
      "16                DeepLearning   \n",
      "17                DeepLearning   \n",
      "18                DeepLearning   \n",
      "19                DeepLearning   \n",
      "20           XGBoost_Heavy_CPU   \n",
      "21                DeepLearning   \n",
      "22                DeepLearning   \n",
      "23                DeepLearning   \n",
      "24                DeepLearning   \n",
      "25                DeepLearning   \n",
      "26             XGBoost_Extreme   \n",
      "27                RandomForest   \n",
      "28                     XGBoost   \n",
      "29                RandomForest   \n",
      "30                    LightGBM   \n",
      "31                     XGBoost   \n",
      "32             XGBoost_Extreme   \n",
      "33                     XGBoost   \n",
      "34                 LogisticReg   \n",
      "35                     XGBoost   \n",
      "36                     XGBoost   \n",
      "37                     XGBoost   \n",
      "38                     XGBoost   \n",
      "39                     XGBoost   \n",
      "40                    CatBoost   \n",
      "\n",
      "                                                                                                         Details  \\\n",
      "Rang Global                                                                                                        \n",
      "1                                                           L:[2048, 1024, 512] | Opt:adam | Act:gelu | Drop:0.2   \n",
      "2                                                           L:[2048, 1024, 512] | Opt:adam | Act:relu | Drop:0.2   \n",
      "3                                                                 L:[1024, 512] | Opt:adam | Act:gelu | Drop:0.2   \n",
      "4                                                           L:[2048, 1024, 512] | Opt:adam | Act:gelu | Drop:0.5   \n",
      "5                                                                 L:[1024, 512] | Opt:adam | Act:relu | Drop:0.2   \n",
      "6                                                           L:[2048, 1024, 512] | Opt:adam | Act:relu | Drop:0.5   \n",
      "7                                                                 L:[1024, 512] | Opt:adam | Act:gelu | Drop:0.5   \n",
      "8                                                                  L:[512, 256] | Opt:adam | Act:gelu | Drop:0.2   \n",
      "9                                                                  L:[512, 256] | Opt:adam | Act:relu | Drop:0.2   \n",
      "10                                                                L:[1024, 512] | Opt:adam | Act:relu | Drop:0.5   \n",
      "11                                                             L:[1024, 512] | Opt:rmsprop | Act:gelu | Drop:0.5   \n",
      "12                                                                 L:[512, 256] | Opt:adam | Act:gelu | Drop:0.5   \n",
      "13                                                       L:[2048, 1024, 512] | Opt:rmsprop | Act:gelu | Drop:0.5   \n",
      "14                                                       L:[2048, 1024, 512] | Opt:rmsprop | Act:gelu | Drop:0.2   \n",
      "15                                                       L:[2048, 1024, 512] | Opt:rmsprop | Act:relu | Drop:0.5   \n",
      "16                                                                 L:[512, 256] | Opt:adam | Act:relu | Drop:0.5   \n",
      "17                                                             L:[1024, 512] | Opt:rmsprop | Act:gelu | Drop:0.2   \n",
      "18                                                       L:[2048, 1024, 512] | Opt:rmsprop | Act:relu | Drop:0.2   \n",
      "19                                                             L:[1024, 512] | Opt:rmsprop | Act:relu | Drop:0.5   \n",
      "20                                                                                Depth:10 | Est:300 | RAM:128Go   \n",
      "21                                                             L:[1024, 512] | Opt:rmsprop | Act:relu | Drop:0.2   \n",
      "22                                                              L:[512, 256] | Opt:rmsprop | Act:gelu | Drop:0.2   \n",
      "23                                                              L:[512, 256] | Opt:rmsprop | Act:gelu | Drop:0.5   \n",
      "24                                                              L:[512, 256] | Opt:rmsprop | Act:relu | Drop:0.5   \n",
      "25                                                              L:[512, 256] | Opt:rmsprop | Act:relu | Drop:0.2   \n",
      "26           XGB_Depth8_ColSample | {'n_estimators': 300, 'max_depth': 8, 'learning_rate': 0.05, 'colsample_b...   \n",
      "27                                                                                               N=200 | Entropy   \n",
      "28                                 {'n_estimators': 150, 'max_depth': 6, 'learning_rate': 0.2, 'subsample': 0.8}   \n",
      "29                                                                                               N=100 | Default   \n",
      "30                                                                                       GPU | N=150 | Leaves=31   \n",
      "31                                 {'n_estimators': 100, 'max_depth': 6, 'learning_rate': 0.2, 'subsample': 0.8}   \n",
      "32           XGB_500_SlowLearn | {'n_estimators': 500, 'max_depth': 6, 'learning_rate': 0.03, 'colsample_bytr...   \n",
      "33                                 {'n_estimators': 150, 'max_depth': 6, 'learning_rate': 0.1, 'subsample': 0.8}   \n",
      "34                                                                                                      Baseline   \n",
      "35                                 {'n_estimators': 100, 'max_depth': 6, 'learning_rate': 0.1, 'subsample': 0.8}   \n",
      "36                                 {'n_estimators': 150, 'max_depth': 4, 'learning_rate': 0.2, 'subsample': 0.8}   \n",
      "37                                 {'n_estimators': 100, 'max_depth': 4, 'learning_rate': 0.2, 'subsample': 0.8}   \n",
      "38                                 {'n_estimators': 150, 'max_depth': 4, 'learning_rate': 0.1, 'subsample': 0.8}   \n",
      "39                                 {'n_estimators': 100, 'max_depth': 4, 'learning_rate': 0.1, 'subsample': 0.8}   \n",
      "40                                                                                         GPU | N=150 | Depth=6   \n",
      "\n",
      "             F1-Score         Temps  \n",
      "Rang Global                          \n",
      "1            0.914124     55.746684  \n",
      "2            0.909223     57.704077  \n",
      "3            0.898055     58.309646  \n",
      "4            0.896616     55.682217  \n",
      "5            0.894012     76.741786  \n",
      "6            0.878822     56.243344  \n",
      "7            0.876032     56.402972  \n",
      "8            0.871171     54.539268  \n",
      "9            0.859674     54.979726  \n",
      "10           0.851672     57.948211  \n",
      "11           0.833182     56.645866  \n",
      "12           0.821772     54.903606  \n",
      "13           0.821750     55.703519  \n",
      "14           0.817717     55.509129  \n",
      "15           0.814265     56.521554  \n",
      "16           0.801429     55.407437  \n",
      "17           0.793542     56.680014  \n",
      "18           0.790641     55.698065  \n",
      "19           0.785280     56.513105  \n",
      "20           0.765149  21596.595834  \n",
      "21           0.763385     58.937616  \n",
      "22           0.763060     54.712121  \n",
      "23           0.761383     54.819009  \n",
      "24           0.754779     54.495977  \n",
      "25           0.754260     57.574389  \n",
      "26           0.731539    999.126112  \n",
      "27           0.724154   1874.213511  \n",
      "28           0.712032    308.897644  \n",
      "29           0.710077    309.821128  \n",
      "30           0.695809    393.099905  \n",
      "31           0.675220    213.435299  \n",
      "32           0.664067    921.352189  \n",
      "33           0.663243    314.057100  \n",
      "34           0.629783   1148.110740  \n",
      "35           0.627065    219.262397  \n",
      "36           0.621142    189.144520  \n",
      "37           0.588989    137.187219  \n",
      "38           0.575696    197.923569  \n",
      "39           0.546098    142.093629  \n",
      "40           0.482448     49.473882  \n",
      "\n",
      "ü•á LE CHAMPION EST : DeepLearning\n",
      "   -> Score F1 : 0.9141\n",
      "   -> Config   : L:[2048, 1024, 512] | Opt:adam | Act:gelu | Drop:0.2\n"
     ]
    }
   ],
   "source": [
    "# CLASSEMENT GENERAL FINAL (TOUT CONFONDU) \n",
    "print(\"\\nüèÜ  GRAND VAINQUEUR TOUTES CATEGORIES \")\n",
    "\n",
    "if ALL_RESULTS:\n",
    "    df_final = pd.DataFrame(ALL_RESULTS)\n",
    "    df_final = df_final.sort_values(by=\"F1-Score\", ascending=False).reset_index(drop=True)\n",
    "    df_final.index += 1\n",
    "    df_final.index.name = 'Rang Global'\n",
    "    \n",
    "    print(df_final[['Famille', 'Details', 'F1-Score', 'Temps']])\n",
    "    \n",
    "    path_final = os.path.join(OUTPUT_DIR, \"benchmark_complet_final.csv\")\n",
    "    df_final.to_csv(path_final, index=True)\n",
    "    \n",
    "    best = df_final.iloc[0]\n",
    "    print(f\"\\nü•á LE CHAMPION EST : {best['Famille']}\")\n",
    "    print(f\"   -> Score F1 : {best['F1-Score']:.4f}\")\n",
    "    print(f\"   -> Config   : {best['Details']}\")\n",
    "else:\n",
    "    print(\"‚ùå Liste vide.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masterclass_tooling",
   "language": "python",
   "name": "masterclass_tooling"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "851402ef-d50d-4dac-917f-c4c7af3e4e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåô D√âMARRAGE MODE NUIT (ConvNeXt Tiny) SUR : cuda\n",
      "üõ°Ô∏è S√âCURIT√âS : Anti-Crash + Early Stopping (5 √©poques) + Workers=0 (Anti-Freeze)\n",
      "üîß Initialisation ConvNeXt Tiny...\n",
      "üî• D√âMARRAGE DU MARATHON (100 √©poques max)...\n",
      "   ‚è≥ Ep 1 | Batch 2100 | Loss: 1.7143\n",
      "‚úÖ FIN EP 1/100 | Time: 602s | F1 Val: 0.6470 | LR: 5.0e-05 | üíæ SAUV√â (NEW RECORD)\n",
      "   ‚è≥ Ep 2 | Batch 2100 | Loss: 1.7966\n",
      "‚úÖ FIN EP 2/100 | Time: 595s | F1 Val: 0.6796 | LR: 5.0e-05 | üíæ SAUV√â (NEW RECORD)\n",
      "   ‚è≥ Ep 3 | Batch 2100 | Loss: 1.3312\n",
      "‚úÖ FIN EP 3/100 | Time: 596s | F1 Val: 0.6928 | LR: 5.0e-05 | üíæ SAUV√â (NEW RECORD)\n",
      "   ‚è≥ Ep 4 | Batch 2100 | Loss: 1.3248\n",
      "‚úÖ FIN EP 4/100 | Time: 599s | F1 Val: 0.6973 | LR: 5.0e-05 | üíæ SAUV√â (NEW RECORD)\n",
      "   ‚è≥ Ep 5 | Batch 2100 | Loss: 1.0885\n",
      "‚úÖ FIN EP 5/100 | Time: 592s | F1 Val: 0.7020 | LR: 5.0e-05 | üíæ SAUV√â (NEW RECORD)\n",
      "   ‚è≥ Ep 6 | Batch 2100 | Loss: 1.0652\n",
      "‚úÖ FIN EP 6/100 | Time: 592s | F1 Val: 0.6987 | LR: 5.0e-05 | ‚ö†Ô∏è Pas mieux (1/5)\n",
      "   ‚è≥ Ep 7 | Batch 2100 | Loss: 1.0657\n",
      "‚úÖ FIN EP 7/100 | Time: 593s | F1 Val: 0.6954 | LR: 5.0e-05 | ‚ö†Ô∏è Pas mieux (2/5)\n",
      "   ‚è≥ Ep 8 | Batch 2100 | Loss: 1.0767\n",
      "‚úÖ FIN EP 8/100 | Time: 594s | F1 Val: 0.6929 | LR: 5.0e-05 | ‚ö†Ô∏è Pas mieux (3/5)\n",
      "   ‚è≥ Ep 9 | Batch 2100 | Loss: 0.9301\n",
      "‚úÖ FIN EP 9/100 | Time: 593s | F1 Val: 0.7018 | LR: 2.5e-05 | ‚ö†Ô∏è Pas mieux (4/5)\n",
      "   ‚è≥ Ep 10 | Batch 2100 | Loss: 0.8616\n",
      "‚úÖ FIN EP 10/100 | Time: 594s | F1 Val: 0.7048 | LR: 2.5e-05 | üíæ SAUV√â (NEW RECORD)\n",
      "   ‚è≥ Ep 11 | Batch 2100 | Loss: 0.8052\n",
      "‚úÖ FIN EP 11/100 | Time: 592s | F1 Val: 0.7055 | LR: 2.5e-05 | üíæ SAUV√â (NEW RECORD)\n",
      "   ‚è≥ Ep 12 | Batch 2100 | Loss: 0.7817\n",
      "‚úÖ FIN EP 12/100 | Time: 592s | F1 Val: 0.7049 | LR: 2.5e-05 | ‚ö†Ô∏è Pas mieux (1/5)\n",
      "   ‚è≥ Ep 13 | Batch 2100 | Loss: 0.7399\n",
      "‚úÖ FIN EP 13/100 | Time: 592s | F1 Val: 0.7057 | LR: 2.5e-05 | üíæ SAUV√â (NEW RECORD)\n",
      "   ‚è≥ Ep 14 | Batch 2100 | Loss: 0.7813\n",
      "‚úÖ FIN EP 14/100 | Time: 593s | F1 Val: 0.7025 | LR: 2.5e-05 | ‚ö†Ô∏è Pas mieux (1/5)\n",
      "   ‚è≥ Ep 15 | Batch 2100 | Loss: 0.8320\n",
      "‚úÖ FIN EP 15/100 | Time: 596s | F1 Val: 0.7064 | LR: 2.5e-05 | üíæ SAUV√â (NEW RECORD)\n",
      "   ‚è≥ Ep 16 | Batch 2100 | Loss: 0.7670\n",
      "‚úÖ FIN EP 16/100 | Time: 594s | F1 Val: 0.7027 | LR: 2.5e-05 | ‚ö†Ô∏è Pas mieux (1/5)\n",
      "   ‚è≥ Ep 17 | Batch 2100 | Loss: 0.8975\n",
      "‚úÖ FIN EP 17/100 | Time: 592s | F1 Val: 0.7035 | LR: 2.5e-05 | ‚ö†Ô∏è Pas mieux (2/5)\n",
      "   ‚è≥ Ep 18 | Batch 2100 | Loss: 0.7316\n",
      "‚úÖ FIN EP 18/100 | Time: 591s | F1 Val: 0.7030 | LR: 2.5e-05 | ‚ö†Ô∏è Pas mieux (3/5)\n",
      "   ‚è≥ Ep 19 | Batch 2100 | Loss: 0.7193\n",
      "‚úÖ FIN EP 19/100 | Time: 592s | F1 Val: 0.7055 | LR: 1.3e-05 | ‚ö†Ô∏è Pas mieux (4/5)\n",
      "   ‚è≥ Ep 20 | Batch 2100 | Loss: 0.6927\n",
      "‚úÖ FIN EP 20/100 | Time: 591s | F1 Val: 0.7071 | LR: 1.3e-05 | üíæ SAUV√â (NEW RECORD)\n",
      "   ‚è≥ Ep 21 | Batch 2100 | Loss: 0.6768\n",
      "‚úÖ FIN EP 21/100 | Time: 594s | F1 Val: 0.7080 | LR: 1.3e-05 | üíæ SAUV√â (NEW RECORD)\n",
      "   ‚è≥ Ep 22 | Batch 2100 | Loss: 0.7226\n",
      "‚úÖ FIN EP 22/100 | Time: 593s | F1 Val: 0.7058 | LR: 1.3e-05 | ‚ö†Ô∏è Pas mieux (1/5)\n",
      "   ‚è≥ Ep 23 | Batch 2100 | Loss: 0.6612\n",
      "‚úÖ FIN EP 23/100 | Time: 592s | F1 Val: 0.7070 | LR: 1.3e-05 | ‚ö†Ô∏è Pas mieux (2/5)\n",
      "   ‚è≥ Ep 24 | Batch 2100 | Loss: 0.7937\n",
      "‚úÖ FIN EP 24/100 | Time: 594s | F1 Val: 0.7054 | LR: 1.3e-05 | ‚ö†Ô∏è Pas mieux (3/5)\n",
      "   ‚è≥ Ep 25 | Batch 2100 | Loss: 0.7337\n",
      "‚úÖ FIN EP 25/100 | Time: 593s | F1 Val: 0.7100 | LR: 6.3e-06 | üíæ SAUV√â (NEW RECORD)\n",
      "   ‚è≥ Ep 26 | Batch 2100 | Loss: 0.6824\n",
      "‚úÖ FIN EP 26/100 | Time: 592s | F1 Val: 0.7074 | LR: 6.3e-06 | ‚ö†Ô∏è Pas mieux (1/5)\n",
      "   ‚è≥ Ep 27 | Batch 2100 | Loss: 0.7029\n",
      "‚úÖ FIN EP 27/100 | Time: 593s | F1 Val: 0.7075 | LR: 6.3e-06 | ‚ö†Ô∏è Pas mieux (2/5)\n",
      "   ‚è≥ Ep 28 | Batch 2100 | Loss: 0.7035\n",
      "‚úÖ FIN EP 28/100 | Time: 593s | F1 Val: 0.7102 | LR: 6.3e-06 | üíæ SAUV√â (NEW RECORD)\n",
      "   ‚è≥ Ep 29 | Batch 2100 | Loss: 0.7072\n",
      "‚úÖ FIN EP 29/100 | Time: 593s | F1 Val: 0.7087 | LR: 6.3e-06 | ‚ö†Ô∏è Pas mieux (1/5)\n",
      "   ‚è≥ Ep 30 | Batch 2100 | Loss: 0.6715\n",
      "‚úÖ FIN EP 30/100 | Time: 591s | F1 Val: 0.7075 | LR: 6.3e-06 | ‚ö†Ô∏è Pas mieux (2/5)\n",
      "   ‚è≥ Ep 31 | Batch 2100 | Loss: 0.7019\n",
      "‚úÖ FIN EP 31/100 | Time: 592s | F1 Val: 0.7086 | LR: 6.3e-06 | ‚ö†Ô∏è Pas mieux (3/5)\n",
      "   ‚è≥ Ep 32 | Batch 2100 | Loss: 0.6592\n",
      "‚úÖ FIN EP 32/100 | Time: 591s | F1 Val: 0.7074 | LR: 3.1e-06 | ‚ö†Ô∏è Pas mieux (4/5)\n",
      "   ‚è≥ Ep 33 | Batch 2100 | Loss: 0.6666\n",
      "‚úÖ FIN EP 33/100 | Time: 594s | F1 Val: 0.7076 | LR: 3.1e-06 | ‚ö†Ô∏è Pas mieux (5/5)\n",
      "\n",
      "üõë ARR√äT AUTOMATIQUE : Le score stagne depuis 5 √©poques.\n",
      "   -> Meilleur score final : 0.7102\n",
      "üèÜ NUIT TERMIN√âE. R√©sultat Final : 0.7102\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "#  CONFIGURATION \"NUIT BLANCHE\" \n",
    "PROJECT_ROOT = r\"C:\\Users\\amisf\\Desktop\\datascientest_projet\"\n",
    "IMG_DIR = r\"C:\\Users\\amisf\\Desktop\\datascientest_projet\\data\\raw\\images\\images\\image_train\"\n",
    "OUTPUT_DIR = os.path.join(PROJECT_ROOT, \"implementation\", \"outputs\")\n",
    "BATCH_SIZE = 32\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "scaler = torch.amp.GradScaler('cuda')\n",
    "\n",
    "print(f\"üåô D√âMARRAGE MODE NUIT (ConvNeXt Tiny) SUR : {DEVICE}\")\n",
    "print(\"üõ°Ô∏è S√âCURIT√âS : Anti-Crash + Early Stopping (5 √©poques) + Workers=0 (Anti-Freeze)\")\n",
    "\n",
    "# --- 2. DATASET ---\n",
    "csv_path = os.path.join(PROJECT_ROOT, \"data\", \"raw\")\n",
    "df_x = pd.read_csv(os.path.join(csv_path, \"X_train_update.csv\"), index_col=0)\n",
    "df_y = pd.read_csv(os.path.join(csv_path, \"Y_train_CVw08PX.csv\"), index_col=0)\n",
    "df = pd.merge(df_x, df_y, left_index=True, right_index=True)\n",
    "df['path'] = df.apply(lambda x: os.path.join(IMG_DIR, f\"image_{x['imageid']}_product_{x['productid']}.jpg\"), axis=1)\n",
    "\n",
    "le = LabelEncoder()\n",
    "df['label_encoded'] = le.fit_transform(df['prdtypecode'])\n",
    "NUM_CLASSES = len(le.classes_)\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, stratify=df['label_encoded'], random_state=42)\n",
    "\n",
    "class ConvNextDataset(Dataset):\n",
    "    def __init__(self, df, mode='train'):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.mode = mode\n",
    "        self.transform_train = transforms.Compose([\n",
    "            transforms.Resize((256, 256)),      \n",
    "            transforms.RandomCrop(224),         \n",
    "            transforms.RandomHorizontalFlip(p=0.5), \n",
    "            transforms.RandomRotation(15),      \n",
    "            transforms.ColorJitter(brightness=0.2, contrast=0.2), \n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        self.transform_val = transforms.Compose([\n",
    "            transforms.Resize((256, 256)),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    def __len__(self): return len(self.df)\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        try: img = Image.open(row['path']).convert(\"RGB\")\n",
    "        except: img = Image.new('RGB', (224, 224), (0, 0, 0))\n",
    "        if self.mode == 'train': return self.transform_train(img), torch.tensor(row['label_encoded'], dtype=torch.long)\n",
    "        else: return self.transform_val(img), torch.tensor(row['label_encoded'], dtype=torch.long)\n",
    "\n",
    "# CONFIGURATION STABLE OBLIGATOIRE \n",
    "train_loader = DataLoader(ConvNextDataset(train_df, 'train'), batch_size=BATCH_SIZE, shuffle=True, num_workers=0, pin_memory=True)\n",
    "val_loader = DataLoader(ConvNextDataset(val_df, 'val'), batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "# MODELE\n",
    "print(\"üîß Initialisation ConvNeXt Tiny...\")\n",
    "model = models.convnext_tiny(weights=\"IMAGENET1K_V1\")\n",
    "n_inputs = model.classifier[2].in_features\n",
    "model.classifier[2] = nn.Sequential(\n",
    "    nn.BatchNorm1d(n_inputs),\n",
    "    nn.Linear(n_inputs, 1024),\n",
    "    nn.GELU(),\n",
    "    nn.Dropout(0.3),\n",
    "    nn.Linear(1024, NUM_CLASSES)\n",
    ")\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "# --- 4. PARAMETRES ---\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1) \n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.01) \n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2)\n",
    "\n",
    "# --- 5. BOUCLE PRINCIPALE ---\n",
    "MAX_EPOCHS = 100\n",
    "PATIENCE_LIMIT = 5\n",
    "\n",
    "best_f1 = 0.0\n",
    "patience_counter = 0\n",
    "\n",
    "print(f\"üî• D√âMARRAGE DU MARATHON ({MAX_EPOCHS} √©poques max)...\")\n",
    "\n",
    "for epoch in range(MAX_EPOCHS):\n",
    "    model.train()\n",
    "    loss_ep = 0.0\n",
    "    t0 = time.time()\n",
    "    \n",
    "    for i, (imgs, lbls) in enumerate(train_loader):\n",
    "        imgs, lbls = imgs.to(DEVICE, non_blocking=True), lbls.to(DEVICE, non_blocking=True)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        with torch.amp.autocast('cuda'):\n",
    "            out = model(imgs)\n",
    "            loss = criterion(out, lbls)\n",
    "        \n",
    "        if torch.isnan(loss):\n",
    "            print(f\"\\n‚ùå ARR√äT D'URGENCE : Loss NaN √† l'√©poque {epoch+1}.\")\n",
    "            exit()\n",
    "            \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        loss_ep += loss.item()\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print(f\"   ‚è≥ Ep {epoch+1} | Batch {i} | Loss: {loss.item():.4f}\", end=\"\\r\")\n",
    "\n",
    "    # VALIDATION\n",
    "    model.eval()\n",
    "    preds, targets = [], []\n",
    "    with torch.no_grad():\n",
    "        for imgs, lbls in val_loader:\n",
    "            imgs = imgs.to(DEVICE, non_blocking=True)\n",
    "            with torch.amp.autocast('cuda'):\n",
    "                out = model(imgs)\n",
    "            _, p = torch.max(out, 1)\n",
    "            preds.extend(p.cpu().numpy())\n",
    "            targets.extend(lbls.cpu().numpy())\n",
    "    \n",
    "    val_f1 = f1_score(targets, preds, average='weighted')\n",
    "    duree = time.time() - t0\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    status = \"üí§\"\n",
    "    if val_f1 > best_f1:\n",
    "        best_f1 = val_f1\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), os.path.join(OUTPUT_DIR, \"livrable_model_convnext_NIGHT_BEST.pth\"))\n",
    "        \n",
    "        meta_data = {\"epoch\": epoch+1, \"f1_score\": best_f1, \"model\": \"ConvNeXt Tiny\"}\n",
    "        with open(os.path.join(OUTPUT_DIR, \"night_run_log.json\"), 'w') as f: json.dump(meta_data, f)\n",
    "        status = \"üíæ SAUV√â (NEW RECORD)\"\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        status = f\"‚ö†Ô∏è Pas mieux ({patience_counter}/{PATIENCE_LIMIT})\"\n",
    "\n",
    "    print(f\"\\n‚úÖ FIN EP {epoch+1}/{MAX_EPOCHS} | Time: {duree:.0f}s | F1 Val: {val_f1:.4f} | LR: {current_lr:.1e} | {status}\")\n",
    "    \n",
    "    scheduler.step(val_f1)\n",
    "    \n",
    "    if patience_counter >= PATIENCE_LIMIT:\n",
    "        print(f\"\\nüõë ARR√äT AUTOMATIQUE : Le score stagne depuis {PATIENCE_LIMIT} √©poques.\")\n",
    "        print(f\"   -> Meilleur score final : {best_f1:.4f}\")\n",
    "        break\n",
    "\n",
    "print(f\"üèÜ NUIT TERMIN√âE. R√©sultat Final : {best_f1:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masterclass_tooling",
   "language": "python",
   "name": "masterclass_tooling"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
